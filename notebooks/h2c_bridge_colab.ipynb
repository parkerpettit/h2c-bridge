{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# H2C Bridge - Colab Development Notebook"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Cloning into '/content/h2c-bridge'...\n",
                        "remote: Enumerating objects: 55, done.\u001b[K\n",
                        "remote: Counting objects: 100% (55/55), done.\u001b[K\n",
                        "remote: Compressing objects: 100% (43/43), done.\u001b[K\n",
                        "remote: Total 55 (delta 10), reused 53 (delta 8), pack-reused 0 (from 0)\u001b[K\n",
                        "Receiving objects: 100% (55/55), 91.31 KiB | 13.04 MiB/s, done.\n",
                        "Resolving deltas: 100% (10/10), done.\n",
                        "✅ Repository cloned\n",
                        "/content/h2c-bridge\n",
                        "Working directory: /content/h2c-bridge\n"
                    ]
                }
            ],
            "source": [
                "# Clone repository\n",
                "REPO_URL = \"https://github.com/parkerpettit/h2c-bridge.git\" \n",
                "\n",
                "import os\n",
                "if not os.path.exists(\"/content/h2c-bridge\"):\n",
                "    !git clone $REPO_URL /content/h2c-bridge\n",
                "    print(\"✅ Repository cloned\")\n",
                "else:\n",
                "    print(\"Repository already exists, pulling latest changes...\")\n",
                "    !cd /content/h2c-bridge && git pull\n",
                "\n",
                "%cd /content/h2c-bridge\n",
                "print(f\"Working directory: {os.getcwd()}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
                        "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m46.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
                        "\u001b[?25hPackage installed\n"
                    ]
                }
            ],
            "source": [
                "# Install package in editable mode\n",
                "!pip install -q -e .\n",
                "!pip install -q -U bitsandbytes\n",
                "print(\"Package installed\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
                        "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
                        "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
                        "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
                        "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mppettit\u001b[0m (\u001b[33mppettit_nlp\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "✓ Logged in as: ppettit_nlp\n"
                    ]
                }
            ],
            "source": [
                "import wandb\n",
                "import getpass\n",
                "import os\n",
                "\n",
                "api_key = getpass.getpass('WandB API Key: ')\n",
                "os.environ['WANDB_API_KEY'] = api_key\n",
                "wandb.login(key=api_key)\n",
                "print(f\"✓ Logged in as: {wandb.api.viewer()['entity']}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n",
                        "WARNING:huggingface_hub._login:Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "✓ Logged in to HuggingFace\n"
                    ]
                }
            ],
            "source": [
                "from huggingface_hub import login\n",
                "\n",
                "token = getpass.getpass('HF Token: ')\n",
                "os.environ['HF_TOKEN'] = token\n",
                "login(token=token)\n",
                "print(\"✓ Logged in to HuggingFace\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Initialize Components\n",
                "\n",
                "Set up models, data, and configuration. Edit `config.py` or `factory.py` and re-run this cell to test changes."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Random seed set to 42\n",
                        "Initializing Factory...\n",
                        "--- [ModelFactory] Loading Tokenizers...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
                        "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
                        "You are not authenticated with the Hugging Face Hub in this notebook.\n",
                        "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
                        "  warnings.warn(\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "6f9b3ea964724d5f834f8fdd9b236209",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "tokenizer_config.json:   0%|          | 0.00/55.4k [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "0eace5624457463b93b145fcfe2d256c",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "d125e3e33e314c149b1e543365ffc791",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "cd78666bcf2240d69bfeb6efd12bdc02",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "tokenizer_config.json: 0.00B [00:00, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "98ba1b71dc144e3ba132c0d6f73b305f",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "vocab.json: 0.00B [00:00, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "4e9f25414eb94a54ae55e64957defa6b",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "merges.txt: 0.00B [00:00, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "44e9df17dffc4f1f93b67709a6c080ae",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "tokenizer.json: 0.00B [00:00, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Initializing Data Module...\n",
                        "✅ Setup complete\n"
                    ]
                }
            ],
            "source": [
                "from h2c_bridge.config import get_default_config\n",
                "from h2c_bridge.factory import H2CModelFactory\n",
                "from h2c_bridge.data.datamodule import H2CDataModule\n",
                "from h2c_bridge.utils import set_seed, clear_gpu\n",
                "\n",
                "# Set seed for reproducibility\n",
                "set_seed(42)\n",
                "\n",
                "# Get config and customize for development\n",
                "config = get_default_config()\n",
                "config.update({\n",
                "        \"SHARER_ID\": \"meta-llama/Llama-3.1-8B-Instruct\",\n",
                "        \"RECEIVER_ID\": \"Qwen/Qwen2.5-0.5B-Instruct\",\n",
                "        \n",
                "        # Dataset size\n",
                "        \"MAX_SAMPLES\": 100_000,  # max samples of OpenHermes to pretrain bridge on\n",
                "        \"BATCH_SIZE\": 8,\n",
                "        \"lr\": 1e-4,\n",
                "        \n",
                "        # Evaluation frequency (in steps)\n",
                "        \"eval_every\": 1000,\n",
                "        \"log_bridge_every\": 50,  # log bridge gate stats to wandb\n",
                "        \n",
                "        # Training\n",
                "        \"epochs\": 1,\n",
                "        \"gate_warmup_steps\": 0,\n",
                "        \n",
                "        # MMLU evaluation\n",
                "        \"mmlu_sample_size\": 1,  # samples per category (57 categories in validation = 57 total samples)\n",
                "    \n",
                "        # Logging\n",
                "        \"wandb_log_examples\": 10,  # number of examples to log to WandB per eval mode\n",
                "   \n",
                "    })\n",
                "\n",
                "     \n",
                "\n",
                "\n",
                "print(\"Initializing Factory...\")\n",
                "factory = H2CModelFactory(config[\"SHARER_ID\"], config[\"RECEIVER_ID\"])\n",
                "tok_sharer, tok_receiver = factory.load_tokenizers()\n",
                "\n",
                "print(\"Initializing Data Module...\")\n",
                "dm = H2CDataModule(tok_sharer, tok_receiver, config)\n",
                "print(\"✅ Setup complete\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Quick Baseline Check (Optional)\n",
                "\n",
                "Verify the evaluation pipeline works before training."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "Finishing previous runs because reinit is set to True."
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            " View run <strong style=\"color:#cdcd00\">Llama-3.1-8B-Instruct_TO_Qwen2.5-0.5B-Instruct</strong> at: <a href='https://wandb.ai/ppettit_nlp/nlp_project/runs/w1weim0n' target=\"_blank\">https://wandb.ai/ppettit_nlp/nlp_project/runs/w1weim0n</a><br> View project at: <a href='https://wandb.ai/ppettit_nlp/nlp_project' target=\"_blank\">https://wandb.ai/ppettit_nlp/nlp_project</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            "Find logs at: <code>./wandb/run-20251204_010007-w1weim0n/logs</code>"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            "Tracking run with wandb version 0.23.0"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            "Run data is saved locally in <code>/content/h2c-bridge/wandb/run-20251204_010325-i4jnxx2k</code>"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            "Syncing run <strong><a href='https://wandb.ai/ppettit_nlp/nlp_project/runs/i4jnxx2k' target=\"_blank\">Llama-3.1-8B-Instruct_TO_Qwen2.5-0.5B-Instruct</a></strong> to <a href='https://wandb.ai/ppettit_nlp/nlp_project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            " View project at <a href='https://wandb.ai/ppettit_nlp/nlp_project' target=\"_blank\">https://wandb.ai/ppettit_nlp/nlp_project</a>"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            " View run at <a href='https://wandb.ai/ppettit_nlp/nlp_project/runs/i4jnxx2k' target=\"_blank\">https://wandb.ai/ppettit_nlp/nlp_project/runs/i4jnxx2k</a>"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "--- [ModelFactory] Loading LLMs (Frozen + Quantized)...\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "383a877f338f4616ba5c0c03dd62eaa5",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "--- [ModelFactory] Initializing Bridge...\n",
                        "--- [Bridge] Aligning Top 24 layers (Sharer: 32, Receiver: 24)\n",
                        "Total parameters: 52,334,640\n",
                        "Trainable parameters: 52,334,640\n",
                        "Size (MB): 199.6 MB (float32)\n",
                        "--- [Scheduler] Cosine schedule: 12376 total steps, 1237 warmup steps\n",
                        "Running baseline evaluation...\n",
                        "\n",
                        "==============================\n",
                        ">>> RUNNING BASELINES (Deterministic)\n",
                        "[DEBUG MODE] Will stop after 5 samples\n",
                        "==============================\n",
                        "\n",
                        "--- Running Baseline: receiver_only ---\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "07ef1c2f006a479f9aa654cce75a1b7c",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Eval [receiver_only] [DEBUG]:   0%|          | 0/15 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
                        "You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
                        "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "[DEBUG MODE] Stopping early after 8 samples\n",
                        "[receiver_only] Acc: 12.50% | Err: 12.50% | Latency: 0.5ms\n",
                        "[WandB] Logged 8 examples to eval_examples/receiver_only\n",
                        "--- Running Baseline: sharer_only ---\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "200c4daf62da4f61ba28db967f6aec15",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Eval [sharer_only] [DEBUG]:   0%|          | 0/15 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "[DEBUG MODE] Stopping early after 8 samples\n",
                        "[sharer_only] Acc: 0.00% | Err: 100.00% | Latency: 1.0ms\n",
                        "[WandB] Logged 8 examples to eval_examples/sharer_only\n",
                        "--- Running Baseline: text_to_text ---\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "e18b9a38fb974777b6c6b23bb141546b",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Eval [text_to_text] [DEBUG]:   0%|          | 0/15 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "[DEBUG MODE] Stopping early after 8 samples\n",
                        "[text_to_text] Acc: 12.50% | Err: 12.50% | Latency: 3.3ms\n",
                        "[WandB] Logged 8 examples to eval_examples/text_to_text\n"
                    ]
                }
            ],
            "source": [
                "from h2c_bridge.training.engine import H2CEngine\n",
                "\n",
                "# Create engine\n",
                "engine = H2CEngine(factory, dm, config, lr=1e-4, eval_every=100)\n",
                "\n",
                "# Run quick baseline check\n",
                "print(\"Running baseline evaluation...\")\n",
                "baseline_results = engine.mmlu_evaluator.evaluate_baselines(engine.mmlu_loader, debug_mode=True)\n",
                "config[\"BASELINES\"] = baseline_results"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Training\n",
                "\n",
                "Train the bridge. Checkpoints automatically upload to WandB as artifacts with aliases:\n",
                "- `latest`: most recent checkpoint\n",
                "- `best`: highest accuracy\n",
                "- `final`: end of training"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Clear GPU and re-initialize for clean training run\n",
                "# clear_gpu()\n",
                "engine = H2CEngine(factory, dm, config, lr=1e-4, eval_every=100)\n",
                "\n",
                "# Start training\n",
                "engine.run(epochs=1)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Resume from Checkpoint (Optional)\n",
                "\n",
                "Load a checkpoint from WandB artifacts to continue training or run inference."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Example: Load the best checkpoint\n",
                "# Format: \"entity/project/artifact_name:alias\"\n",
                "# You can find this in your WandB UI under Artifacts\n",
                "\n",
                "# ARTIFACT_PATH = \"your-entity/nlp_project/bridge_Llama-3-1-8B-Instruct_TO_Qwen2-5-0-5B-Instruct_checkpoint:best\"\n",
                "# engine.load_checkpoint(ARTIFACT_PATH)\n",
                "# print(\"Checkpoint loaded! You can now continue training or run inference.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Debugging & Analysis\n",
                "\n",
                "Test specific components or run ad-hoc experiments."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test a specific prompt\n",
                "prompt = \"Explain how a CPU works.\"\n",
                "engine.evaluator.generate_demo(prompt, max_new_tokens=100)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check bridge gate statistics\n",
                "stats = engine.bridge.get_gate_stats()\n",
                "print(f\"Key Gate Avg: {stats['key_avg']:.4f}\")\n",
                "print(f\"Value Gate Avg: {stats['value_avg']:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Visualizations (Optional)\n",
                "\n",
                "Generate publication-ready visualizations and upload to WandB."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Uncomment to run full visualization suite\n",
                "# from h2c_bridge.visualization import run_all_visualizations\n",
                "# run_all_visualizations(engine, config, themes=(\"dark\", \"light\"))"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
