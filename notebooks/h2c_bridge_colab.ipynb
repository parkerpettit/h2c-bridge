{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# H2C Bridge - Colab Development Notebook"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Cloning into '/content/h2c-bridge'...\n",
                        "remote: Enumerating objects: 74, done.\u001b[K\n",
                        "remote: Counting objects: 100% (74/74), done.\u001b[K\n",
                        "remote: Compressing objects: 100% (59/59), done.\u001b[K\n",
                        "remote: Total 74 (delta 23), reused 62 (delta 11), pack-reused 0 (from 0)\u001b[K\n",
                        "Receiving objects: 100% (74/74), 99.96 KiB | 19.99 MiB/s, done.\n",
                        "Resolving deltas: 100% (23/23), done.\n",
                        "✅ Repository cloned\n",
                        "/content/h2c-bridge\n",
                        "Working directory: /content/h2c-bridge\n"
                    ]
                }
            ],
            "source": [
                "# Clone repository\n",
                "REPO_URL = \"https://github.com/parkerpettit/h2c-bridge.git\" \n",
                "\n",
                "import os\n",
                "if not os.path.exists(\"/content/h2c-bridge\"):\n",
                "    !git clone $REPO_URL /content/h2c-bridge\n",
                "    print(\"✅ Repository cloned\")\n",
                "else:\n",
                "    print(\"Repository already exists, pulling latest changes...\")\n",
                "    !cd /content/h2c-bridge && git pull\n",
                "\n",
                "%cd /content/h2c-bridge\n",
                "print(f\"Working directory: {os.getcwd()}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
                        "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m45.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
                        "\u001b[?25hPackage installed\n"
                    ]
                }
            ],
            "source": [
                "# Install package in editable mode\n",
                "!pip install -q -e .\n",
                "!pip install -q -U bitsandbytes\n",
                "print(\"Package installed\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
                        "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
                        "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
                        "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
                        "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mppettit\u001b[0m (\u001b[33mppettit_nlp\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "✓ Logged in as: ppettit_nlp\n"
                    ]
                }
            ],
            "source": [
                "import wandb\n",
                "import getpass\n",
                "import os\n",
                "\n",
                "api_key = getpass.getpass('WandB API Key: ')\n",
                "os.environ['WANDB_API_KEY'] = api_key\n",
                "wandb.login(key=api_key)\n",
                "print(f\"✓ Logged in as: {wandb.api.viewer()['entity']}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n",
                        "WARNING:huggingface_hub._login:Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "✓ Logged in to HuggingFace\n"
                    ]
                }
            ],
            "source": [
                "from huggingface_hub import login\n",
                "\n",
                "token = getpass.getpass('HF Token: ')\n",
                "os.environ['HF_TOKEN'] = token\n",
                "login(token=token)\n",
                "print(\"✓ Logged in to HuggingFace\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Initialize Components\n",
                "\n",
                "Set up models, data, and configuration. Edit `config.py` or `factory.py` and re-run this cell to test changes."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Random seed set to 42\n",
                        "Initializing Factory...\n",
                        "--- [ModelFactory] Loading Tokenizers...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
                        "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
                        "You are not authenticated with the Hugging Face Hub in this notebook.\n",
                        "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
                        "  warnings.warn(\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "e50c1ccce8e743fdb1f0eb659ee6ae81",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "tokenizer_config.json:   0%|          | 0.00/55.4k [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "9dcf7915138e4b9b883e4616bdbdcdd4",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "3b694d2213f84c01933219cf306457d2",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "be19592ec51a415c8bb324c481e110c2",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "tokenizer_config.json: 0.00B [00:00, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "2223a184cdb440bcb441a6ac1b9e5f64",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "vocab.json: 0.00B [00:00, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "331adddb3ac1487694f779a19f0efb99",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "merges.txt: 0.00B [00:00, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "6bd278fc2f3b4f129f7c405d735456cc",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "tokenizer.json: 0.00B [00:00, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Initializing Data Module...\n",
                        "✅ Setup complete\n"
                    ]
                }
            ],
            "source": [
                "from h2c_bridge.config import get_default_config\n",
                "from h2c_bridge.factory import H2CModelFactory\n",
                "from h2c_bridge.data.datamodule import H2CDataModule\n",
                "from h2c_bridge.utils import set_seed, clear_gpu\n",
                "\n",
                "# Set seed for reproducibility\n",
                "set_seed(42)\n",
                "\n",
                "# Get config and customize for development\n",
                "config = get_default_config()\n",
                "config.update({\n",
                "        \"SHARER_ID\": \"meta-llama/Llama-3.1-8B-Instruct\",\n",
                "        \"RECEIVER_ID\": \"Qwen/Qwen2.5-0.5B-Instruct\",\n",
                "        \n",
                "        # Dataset size\n",
                "        \"MAX_SAMPLES\": 100_000,  # max samples of OpenHermes to pretrain bridge on\n",
                "        \"BATCH_SIZE\": 1,\n",
                "        \"lr\": 1e-4,\n",
                "        \n",
                "        # Evaluation frequency (in steps)\n",
                "        \"eval_every\": 1000,\n",
                "        \"log_bridge_every\": 50,  # log bridge gate stats to wandb\n",
                "        \n",
                "        # Training\n",
                "        \"epochs\": 1,\n",
                "        \"gate_warmup_steps\": 0,\n",
                "        \n",
                "        # MMLU evaluation\n",
                "        \"mmlu_sample_size\": 1,  # samples per category (57 categories in validation = 57 total samples)\n",
                "    \n",
                "        # Logging\n",
                "        \"wandb_log_examples\": 10,  # number of examples to log to WandB per eval mode\n",
                "   \n",
                "    })\n",
                "\n",
                "     \n",
                "\n",
                "\n",
                "print(\"Initializing Factory...\")\n",
                "factory = H2CModelFactory(config[\"SHARER_ID\"], config[\"RECEIVER_ID\"])\n",
                "tok_sharer, tok_receiver = factory.load_tokenizers()\n",
                "\n",
                "print(\"Initializing Data Module...\")\n",
                "dm = H2CDataModule(tok_sharer, tok_receiver, config)\n",
                "print(\"✅ Setup complete\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loading OpenHermes-2.5 (train)...\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "61351b70602d4909908a10c83ffef330",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "README.md: 0.00B [00:00, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "b6590f5fac4f4ca8a8673905b41d5953",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "openhermes2_5.json:   0%|          | 0.00/1.94G [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "ca29c1e0bad24352aaee7dbb703b7673",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Generating train split:   0%|          | 0/1001551 [00:00<?, ? examples/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Processed 994 valid conversation pairs (Skipped 6 > 2048 tokens).\n"
                    ]
                }
            ],
            "source": [
                "from h2c_bridge.data.datasets import H2CDatasetWrapper\n",
                "\n",
                "ds = H2CDatasetWrapper(split=\"train\", max_samples=1000, tokenizer=tok_sharer, max_len=2048)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Every day, a tree drops 7 leaves. How many leaves would it drop in a month of February in a non-leap year? Include your logic.\n"
                    ]
                }
            ],
            "source": [
                "\n",
                "print(ds[0][\"prompt\"])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": [
                "# from h2c_bridge.config import get_default_config\n",
                "# from h2c_bridge.factory import H2CModelFactory\n",
                "# from h2c_bridge.data.datamodule import H2CDataModule\n",
                "\n",
                "# def inspect_tokens():\n",
                "#     print(\">>> Loading Configuration and Models...\")\n",
                "#     config = get_default_config()\n",
                "    \n",
                "#     # Initialize Factory\n",
                "#     factory = H2CModelFactory(config[\"SHARER_ID\"], config[\"RECEIVER_ID\"])\n",
                "#     tok_sharer, tok_receiver = factory.load_tokenizers()\n",
                "    \n",
                "#     print(f\"Sharer Tokenizer: {tok_sharer.name_or_path}\")\n",
                "#     print(f\"Receiver Tokenizer: {tok_receiver.name_or_path}\")\n",
                "\n",
                "#     # Initialize DataModule\n",
                "#     print(\">>> Loading DataModule...\")\n",
                "#     dm = H2CDataModule(tok_sharer, tok_receiver, config)\n",
                "#     dm.setup()\n",
                "    \n",
                "#     train_loader, _ = dm.get_loaders()\n",
                "    \n",
                "#     print(\">>> Fetching one batch...\")\n",
                "#     batch = next(iter(train_loader))\n",
                "    \n",
                "#     # Inspect Sharer Input\n",
                "#     print(\"\\n\" + \"=\"*50)\n",
                "#     print(\"SHARER INPUT (First Example)\")\n",
                "#     print(\"=\"*50)\n",
                "#     sharer_ids = batch['sharer_input_ids'][0]\n",
                "#     # Decode with special tokens visible\n",
                "#     sharer_text = tok_sharer.decode(sharer_ids, skip_special_tokens=False)\n",
                "#     print(sharer_text)\n",
                "    \n",
                "#     # Inspect Receiver Input (Prompt + Target)\n",
                "#     print(\"\\n\" + \"=\"*50)\n",
                "#     print(\"RECEIVER INPUT (First Example)\")\n",
                "#     print(\"=\"*50)\n",
                "#     rec_prompt_ids = batch['receiver_prompt_ids'][0]\n",
                "#     rec_kickstart_ids = batch['receiver_kickstart_ids'][0]\n",
                "#     rec_target_ids = batch['receiver_target_ids'][0]\n",
                "    \n",
                "#     # Combine for full view\n",
                "#     full_ids = list(rec_prompt_ids) + list(rec_kickstart_ids) + list(rec_target_ids)\n",
                "    \n",
                "#     # Decode with special tokens visible\n",
                "#     rec_text = tok_receiver.decode(full_ids, skip_special_tokens=False)\n",
                "#     print(rec_text)\n",
                "\n",
                "# # inspect_tokens()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Quick Baseline Check (Optional)\n",
                "\n",
                "Verify the evaluation pipeline works before training."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.\n"
                    ]
                },
                {
                    "data": {
                        "text/html": [],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            "Tracking run with wandb version 0.23.0"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            "Run data is saved locally in <code>/content/h2c-bridge/wandb/run-20251204_023333-jdz6k7xp</code>"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            "Syncing run <strong><a href='https://wandb.ai/ppettit_nlp/nlp_project/runs/jdz6k7xp' target=\"_blank\">Llama-3.1-8B-Instruct_TO_Qwen2.5-0.5B-Instruct</a></strong> to <a href='https://wandb.ai/ppettit_nlp/nlp_project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            " View project at <a href='https://wandb.ai/ppettit_nlp/nlp_project' target=\"_blank\">https://wandb.ai/ppettit_nlp/nlp_project</a>"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            " View run at <a href='https://wandb.ai/ppettit_nlp/nlp_project/runs/jdz6k7xp' target=\"_blank\">https://wandb.ai/ppettit_nlp/nlp_project/runs/jdz6k7xp</a>"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "--- [ModelFactory] Loading LLMs (Frozen + Quantized)...\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "6142c7a6da44421189309317e75b981a",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "config.json:   0%|          | 0.00/855 [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "d3520e7c614e49b78949319acd9d58f5",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "622185ef3f484e68a974ba2827059b31",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "9f0a73b971314de5aa96cf7045b94fb0",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "7d7aac876dd64de6b387219672eaadbc",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "ec9b7d292385489fa12d8a9976061b6b",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "95f60d8710a84eb4a50a09596e42136b",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "5f567e7d436a44979b835bc9e394fb1e",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "220dbcaa96ee4a009bf768bd96eaada2",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "generation_config.json:   0%|          | 0.00/184 [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "26145acef79d4d1d8fa8e057aad27f5a",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "config.json:   0%|          | 0.00/659 [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "9268012145fe467889756e7540cf6253",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "model.safetensors:   0%|          | 0.00/988M [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "7258c729b43e49a8a8b09e70e90b4b17",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "generation_config.json:   0%|          | 0.00/242 [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "--- [ModelFactory] Initializing Bridge...\n",
                        "--- [Bridge] Aligning Top 24 layers (Sharer: 32, Receiver: 24)\n",
                        "Total parameters: 52,334,640\n",
                        "Trainable parameters: 52,334,640\n",
                        "Size (MB): 199.6 MB (float32)\n",
                        "--- [DataModule] Loading Datasets (Max 100000)...\n",
                        "Loading OpenHermes-2.5 (train)...\n",
                        "Processed 99663 valid conversation pairs (Skipped 337 > 2048 tokens).\n",
                        "--- [MMLU] Loading auxiliary_train split...\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "94101d6871d1481aaf003be12d800754",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "README.md: 0.00B [00:00, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "54892ddb9a924189bdce377bfba078cd",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "dataset_infos.json: 0.00B [00:00, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "65178f597a4948d48bcdbd43dd7c236f",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "all/test-00000-of-00001.parquet:   0%|          | 0.00/3.50M [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "878f48aea01346019996837ac7388ffe",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "all/validation-00000-of-00001.parquet:   0%|          | 0.00/408k [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "12d5d9ec747e49d39bc049418dcff43c",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "all/dev-00000-of-00001.parquet:   0%|          | 0.00/76.5k [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "5b072367c2194579bbf285540d98ef84",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "all/auxiliary_train-00000-of-00001.parqu(…):   0%|          | 0.00/47.5M [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "cf62af96939d4726953cf262ed945e06",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Generating test split:   0%|          | 0/14042 [00:00<?, ? examples/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "555508fcf3b7498f9165423ebbaaf4a1",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Generating validation split:   0%|          | 0/1531 [00:00<?, ? examples/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "3f3caab1fde343dc9759ce94a0f0e3b3",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Generating dev split:   0%|          | 0/285 [00:00<?, ? examples/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "099349bbdcb6430daaaa4885ecf177d1",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Generating auxiliary_train split:   0%|          | 0/99842 [00:00<?, ? examples/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "--- [MMLU] Found 1 subjects\n",
                        "--- [MMLU] Processed 5 examples (auxiliary_train).\n",
                        "--- [DataModule] Combined Train Source Sizes: 99663 OpenHermes + 5 MMLU Aux = 99668 total\n",
                        "--- [DataModule] Split: 98671 Train | 997 Val\n",
                        "--- [DataModule] Setting up MMLU Eval (Validation Split)...\n",
                        "--- [MMLU] Loading validation split...\n",
                        "--- [MMLU] Found 57 subjects\n",
                        "--- [MMLU] Processed 57 examples (validation).\n",
                        "--- [Scheduler] Cosine schedule: 12334 total steps, 1233 warmup steps\n",
                        "Running baseline evaluation...\n",
                        "\n",
                        "==============================\n",
                        ">>> RUNNING BASELINES (Deterministic)\n",
                        "[DEBUG MODE] Will stop after 5 samples\n",
                        "==============================\n",
                        "\n",
                        "--- Running Baseline: receiver_only ---\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "50d36babc9f94979ba79caf215c22884",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Eval [receiver_only] [DEBUG]:   0%|          | 0/15 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
                        "You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
                        "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "[DEBUG MODE] Stopping early after 8 samples\n",
                        "[receiver_only] Acc: 37.50% | Err: 0.00% | Latency: 1.8ms\n",
                        "[WandB] Logged 8 examples to eval_examples/receiver_only\n",
                        "--- Running Baseline: sharer_only ---\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "433d9b51c4504c66919b23b85d408bf4",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Eval [sharer_only] [DEBUG]:   0%|          | 0/15 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "[DEBUG MODE] Stopping early after 8 samples\n",
                        "[sharer_only] Acc: 62.50% | Err: 0.00% | Latency: 0.6ms\n",
                        "[WandB] Logged 8 examples to eval_examples/sharer_only\n",
                        "--- Running Baseline: text_to_text ---\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "cb90677ac4904fd48860a75ad1bd61e3",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Eval [text_to_text] [DEBUG]:   0%|          | 0/15 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "[DEBUG MODE] Stopping early after 8 samples\n",
                        "[text_to_text] Acc: 12.50% | Err: 0.00% | Latency: 6.3ms\n",
                        "[WandB] Logged 8 examples to eval_examples/text_to_text\n"
                    ]
                }
            ],
            "source": [
                "from h2c_bridge.training.engine import H2CEngine\n",
                "\n",
                "# Create engine\n",
                "engine = H2CEngine(factory, dm, config, lr=1e-4, eval_every=100)\n",
                "\n",
                "# Run quick baseline check\n",
                "print(\"Running baseline evaluation...\")\n",
                "baseline_results = engine.mmlu_evaluator.evaluate_baselines(engine.mmlu_loader, debug_mode=True)\n",
                "config[\"BASELINES\"] = baseline_results"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Training\n",
                "\n",
                "Train the bridge. Checkpoints automatically upload to WandB as artifacts with aliases:\n",
                "- `latest`: most recent checkpoint\n",
                "- `best`: highest accuracy\n",
                "- `final`: end of training"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Cleared GPU cache.\n"
                    ]
                },
                {
                    "data": {
                        "text/html": [
                            "Finishing previous runs because reinit is set to True."
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            " View run <strong style=\"color:#cdcd00\">Llama-3.1-8B-Instruct_TO_Qwen2.5-0.5B-Instruct</strong> at: <a href='https://wandb.ai/ppettit_nlp/nlp_project/runs/jdz6k7xp' target=\"_blank\">https://wandb.ai/ppettit_nlp/nlp_project/runs/jdz6k7xp</a><br> View project at: <a href='https://wandb.ai/ppettit_nlp/nlp_project' target=\"_blank\">https://wandb.ai/ppettit_nlp/nlp_project</a><br>Synced 5 W&B file(s), 3 media file(s), 6 artifact file(s) and 0 other file(s)"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            "Find logs at: <code>./wandb/run-20251204_023333-jdz6k7xp/logs</code>"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            "Tracking run with wandb version 0.23.0"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            "Run data is saved locally in <code>/content/h2c-bridge/wandb/run-20251204_023745-d6266t5b</code>"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            "Syncing run <strong><a href='https://wandb.ai/ppettit_nlp/nlp_project/runs/d6266t5b' target=\"_blank\">Llama-3.1-8B-Instruct_TO_Qwen2.5-0.5B-Instruct</a></strong> to <a href='https://wandb.ai/ppettit_nlp/nlp_project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            " View project at <a href='https://wandb.ai/ppettit_nlp/nlp_project' target=\"_blank\">https://wandb.ai/ppettit_nlp/nlp_project</a>"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            " View run at <a href='https://wandb.ai/ppettit_nlp/nlp_project/runs/d6266t5b' target=\"_blank\">https://wandb.ai/ppettit_nlp/nlp_project/runs/d6266t5b</a>"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "--- [ModelFactory] Loading LLMs (Frozen + Quantized)...\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "95417f2bd0fe45ec9ea6150e24243390",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "--- [ModelFactory] Initializing Bridge...\n",
                        "--- [Bridge] Aligning Top 24 layers (Sharer: 32, Receiver: 24)\n",
                        "Total parameters: 52,334,640\n",
                        "Trainable parameters: 52,334,640\n",
                        "Size (MB): 199.6 MB (float32)\n",
                        "--- [Scheduler] Cosine schedule: 12334 total steps, 1233 warmup steps\n",
                        "--- [Engine] Starting Training for 1 epochs...\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "869b813447244c57b396603f075e39fe",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Epoch 1:   0%|          | 0/12334 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "ename": "OutOfMemoryError",
                    "evalue": "CUDA out of memory. Tried to allocate 2.56 GiB. GPU 0 has a total capacity of 22.16 GiB of which 101.38 MiB is free. Process 21469 has 22.06 GiB memory in use. Of the allocated memory 21.60 GiB is allocated by PyTorch, and 229.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
                        "\u001b[0;32m/tmp/ipython-input-2031481592.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Start training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
                        "\u001b[0;32m/content/h2c-bridge/h2c_bridge/training/engine.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, epochs)\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"--- [Engine] Starting Training for {epochs} epochs...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;31m# Save final checkpoint at end of training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/content/h2c-bridge/h2c_bridge/training/engine.py\u001b[0m in \u001b[0;36m_run_epoch\u001b[0;34m(self, epoch_idx)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprogress_bar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/content/h2c-bridge/h2c_bridge/training/trainer.py\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;31m# 4. Optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;31m# Free computation graph after backward (gradients computed)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             )\n\u001b[0;32m--> 625\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    626\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m         )\n",
                        "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    839\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    842\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
                        "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.56 GiB. GPU 0 has a total capacity of 22.16 GiB of which 101.38 MiB is free. Process 21469 has 22.06 GiB memory in use. Of the allocated memory 21.60 GiB is allocated by PyTorch, and 229.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
                    ]
                }
            ],
            "source": [
                "# Clear GPU and re-initialize for clean training run\n",
                "clear_gpu()\n",
                "engine = H2CEngine(factory, dm, config, lr=1e-4, eval_every=100)\n",
                "\n",
                "# Start training\n",
                "engine.run(epochs=1)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Resume from Checkpoint (Optional)\n",
                "\n",
                "Load a checkpoint from WandB artifacts to continue training or run inference."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Example: Load the best checkpoint\n",
                "# Format: \"entity/project/artifact_name:alias\"\n",
                "# You can find this in your WandB UI under Artifacts\n",
                "\n",
                "# ARTIFACT_PATH = \"your-entity/nlp_project/bridge_Llama-3-1-8B-Instruct_TO_Qwen2-5-0-5B-Instruct_checkpoint:best\"\n",
                "# engine.load_checkpoint(ARTIFACT_PATH)\n",
                "# print(\"Checkpoint loaded! You can now continue training or run inference.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Debugging & Analysis\n",
                "\n",
                "Test specific components or run ad-hoc experiments."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test a specific prompt\n",
                "prompt = \"Explain how a CPU works.\"\n",
                "engine.evaluator.generate_demo(prompt, max_new_tokens=100)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check bridge gate statistics\n",
                "stats = engine.bridge.get_gate_stats()\n",
                "print(f\"Key Gate Avg: {stats['key_avg']:.4f}\")\n",
                "print(f\"Value Gate Avg: {stats['value_avg']:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Visualizations (Optional)\n",
                "\n",
                "Generate publication-ready visualizations and upload to WandB."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Uncomment to run full visualization suite\n",
                "# from h2c_bridge.visualization import run_all_visualizations\n",
                "# run_all_visualizations(engine, config, themes=(\"dark\", \"light\"))"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
