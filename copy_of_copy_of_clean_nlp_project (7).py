# -*- coding: utf-8 -*-
"""Copy of Copy of clean_nlp_project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Wlyt2bKATnKsovqskiYDwlwdE9bBOjt_

# Imports
"""

!pip install -U bitsandbytes

# --- Standard Library ---
import copy
import json
import math
import os
import random
import re
import time
import gc
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Dict, List, Optional, Tuple, Union

# --- Scientific & Logging ---
import numpy as np
import pandas as pd
import seaborn as sns
import wandb
from matplotlib import pyplot as plt
import matplotlib.patheffects as path_effects
from matplotlib.colors import LinearSegmentedColormap
from matplotlib.patches import FancyBboxPatch
from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix
from tqdm.auto import tqdm

# --- PyTorch ---
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch import Tensor
from torch.amp import GradScaler
from torch.utils.data import DataLoader, Dataset, IterableDataset, random_split

# --- Datasets / Hugging Face ---
import datasets
datasets.disable_progress_bar()
from datasets import load_dataset
from transformers import (
    AutoConfig,
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    DynamicCache,
    PreTrainedTokenizer,
    get_cosine_schedule_with_warmup,
)

# --- Environment (Colab) ---
from google.colab import drive, userdata


drive.mount("/content/drive")
os.environ["WANDB_API_KEY"] = userdata.get("wandb")

def set_seed(seed=42): # TO MAKE DETERMINISTIC
    # 1. Python and NumPy
    random.seed(seed)
    np.random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)

    # 2. PyTorch (CPU + GPU)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if using multi-GPU

    # 3. Force Deterministic Algorithms
    # This slows down training slightly but ensures convolution/matmul operations
    # are reproducible.
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

    print(f"Random seed set to {seed}")

"""# Utils

"""

def get_model_dims(model_id):
    """
    Dynamically extracts dimension info from HF AutoConfig.
    """
    config = AutoConfig.from_pretrained(model_id)
    hidden_dim = getattr(config, "hidden_size", getattr(config, "n_embd", None))
    num_heads = getattr(config, "num_attention_heads", getattr(config, "n_head", None))
    kv_heads = getattr(config, "num_key_value_heads", num_heads) # Fallback to num_heads if no GQA
    num_layers = getattr(config, "num_hidden_layers", getattr(config, "n_layer", None))

    # Calculate head_dim if not explicitly stated
    head_dim = getattr(config, "head_dim", None)
    if head_dim is None and hidden_dim and num_heads:
        head_dim = hidden_dim // num_heads

    print(f"[{model_id}] Dims -> Hidden: {hidden_dim}, Layers: {num_layers}, KV Heads: {kv_heads}, Head Dim: {head_dim}")

    return {
        "hidden_dim": hidden_dim,
        "num_layers": num_layers,
        "num_heads": num_heads,
        "kv_heads": kv_heads,
        "head_dim": head_dim,
        "config": config
    }

def clear_gpu():
    """Aggressively clear GPU memory by deleting model globals and emptying cache."""
    # All potential globals that hold GPU tensors
    names_to_delete = [
        "engine", "factory", "dm", "trainer", "evaluator",
        "sharer", "receiver", "bridge",
        "tok_sharer", "tok_receiver",
        "train_loader", "val_loader", "mmlu_loader"
    ]

    g = globals()
    for name in names_to_delete:
        if name in g:
            try:
                obj = g[name]
                # If it has a .to() method, try moving to CPU first
                if hasattr(obj, 'to'):
                    try:
                        obj.to('cpu')
                    except:
                        pass
                del g[name]
            except Exception as e:
                print(f"Warning: Could not delete {name}: {e}")

    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()
    print("Cleared GPU cache.")

"""#Bridge Code"""

class H2CAttentionBlock(nn.Module):
  """
  A single block performing Cross-Attention from Receiver KV (Query)
  to Sharer hidden states (key/value).
  """
  def __init__(
      self,
      sharer_dim: int,          # Dim of sharer hidden states
      receiver_dim: int,        # Dim of receiver hidden states
      num_attention_heads: int, # Number of heads for projection attention (not model heads)
      dropout: float = 0.1,
      dtype: torch.dtype = torch.float32,
  ):
    super().__init__()

    # Pre-norms for stability
    self.norm_receiver = nn.LayerNorm(receiver_dim, dtype=dtype)
    self.norm_sharer = nn.LayerNorm(sharer_dim, dtype=dtype)

    # Cross attention
    self.attn = nn.MultiheadAttention(
        embed_dim=receiver_dim,
        num_heads=num_attention_heads,
        kdim=sharer_dim,
        vdim=sharer_dim,
        dropout=dropout,
        batch_first=True,
        dtype=dtype,
    )


    # self.out_proj = nn.Linear(receiver_dim, receiver_dim, dtype=dtype)
    # Zero-init output projection so bridge starts as identity (no corruption)
    nn.init.normal_(self.attn.out_proj.weight, std=0.01)
    nn.init.zeros_(self.attn.out_proj.bias)

    # Initialize near 0 with noise: sigmoid(0) â‰ˆ 0.5, so gate starts around 0.5
    # Noise allows per-layer variation in initial gate values
    init = 0.1 * torch.randn(1, dtype=dtype)  # Small noise around 0
    self.gate = nn.Parameter(init)
    self.dropout = nn.Dropout(dropout)




  def forward(self, receiver_state: Tensor, sharer_hidden: Tensor,
              ) -> Tensor:
      """
      Args:
          receiver_state: Tensor of shape (B, Seq, receiver_num_heads * receiver_head_dim)
          sharer_hidden: Tensor of shape (B, Seq, sharer_dim)

      """
      # Apply norms
      q = self.norm_receiver(receiver_state)
      k = v = self.norm_sharer(sharer_hidden)

      # Cross attention: Query=Receiver, Key/Val = sharer
      attn_output, _ = self.attn(query=q, key=k, value=v, need_weights=False)

      # Dropout on attention output
      output = self.dropout(attn_output)

      # Simple learned gate in (0, 1)
      gate = torch.sigmoid(self.gate)

      return receiver_state + gate * output


class H2CProjector(nn.Module):
    """
    Hidden-to-Cache projector using Cross Attention.
    """
    def __init__(
        self,
        sharer_dim: int,
        receiver_head_dim: int,
        receiver_num_heads: int,
        num_receiver_layers: int,
        sharer_num_layers: int, # Added argument
        proj_num_heads: int = 4,
        dropout: float = 0.1,
        dtype: torch.dtype = torch.float32,
    ):
        super().__init__()

        self.sharer_dim = sharer_dim
        self.receiver_head_dim = receiver_head_dim
        self.receiver_num_heads = receiver_num_heads
        self.flat_receiver_dim = receiver_head_dim * receiver_num_heads

        # Store layer counts for alignment logic
        self.sharer_num_layers = sharer_num_layers # You need to pass this in __init__
        self.receiver_num_layers = num_receiver_layers

        # Top-Down Alignment: Bridge the top min(N, M) layers
        self.num_bridged_layers = min(self.sharer_num_layers, self.receiver_num_layers)
        print(f"--- [Bridge] Aligning Top {self.num_bridged_layers} layers (Sharer: {self.sharer_num_layers}, Receiver: {self.receiver_num_layers})")

        # Create a separate block for every bridged layer
        self.key_modifiers = nn.ModuleList([
            H2CAttentionBlock(
                sharer_dim=sharer_dim,
                receiver_dim=self.flat_receiver_dim,
                num_attention_heads=proj_num_heads,
                dropout=dropout,
                dtype=dtype
            ) for _ in range(self.num_bridged_layers)
        ])

        self.value_modifiers = nn.ModuleList([
            H2CAttentionBlock(
                sharer_dim=sharer_dim,
                receiver_dim=self.flat_receiver_dim,
                num_attention_heads=proj_num_heads,
                dropout=dropout,
                dtype=dtype
            ) for _ in range(self.num_bridged_layers)
        ])

    def forward(self,
                source_hidden: Tensor,
                target_kv: Tuple[Tensor, Tensor],
                layer_idx: int) -> Tuple[Tensor, Tensor]:
        """
        Calculates projection for a SPECIFIC layer index.
        """
        if layer_idx >= len(self.key_modifiers):
            raise ValueError(f"Layer index {layer_idx} out of bounds for Projector initialized with {len(self.key_modifiers)} layers")

        target_k, target_v = target_kv
        B, H, N, D = target_k.shape

        # Clone immediately to avoid any reference to original cache
        target_k = target_k.clone()
        target_v = target_v.clone()


        # Flatten
        target_k_flat = target_k.transpose(1, 2).reshape(B, N, H*D)
        target_v_flat = target_v.transpose(1, 2).reshape(B, N, H*D)

        # Select the specific blocks for this layer
        key_block = self.key_modifiers[layer_idx]
        value_block = self.value_modifiers[layer_idx]

        # Apply Cross Attention
        modified_k_flat = key_block(target_k_flat, source_hidden)
        modified_v_flat = value_block(target_v_flat, source_hidden)

        # Unflatten
        modified_k = modified_k_flat.view(B, N, H, D).transpose(1, 2)
        modified_v = modified_v_flat.view(B, N, H, D).transpose(1, 2)

        return modified_k, modified_v

    def cache_project(self, source_hidden_states: Tuple[Tensor], target_kv_cache: DynamicCache) -> DynamicCache:
        if not isinstance(target_kv_cache, DynamicCache):
            raise ValueError(f"Expected DynamicCache, got {type(target_kv_cache)}")

        # Shape Assertions
        assert isinstance(source_hidden_states, (tuple, list)), f"Expected tuple/list of hidden states, got {type(source_hidden_states)}"
        assert len(source_hidden_states) >= self.sharer_num_layers, f"Expected at least {self.sharer_num_layers} sharer layers, got {len(source_hidden_states)}"

        projected_cache = DynamicCache()

        # Iterate through Receiver layers (0 to N_R - 1)
        for r_layer_idx in range(len(target_kv_cache)):
            target_kv = target_kv_cache[r_layer_idx]

            # Top-Down Alignment Logic
            # We want to bridge the TOP 'num_bridged_layers'
            # The 'bridge index' 0 corresponds to the TOPMOST bridged layer

            # Calculate how far from the top we are
            layers_from_top = (self.receiver_num_layers - 1) - r_layer_idx

            if layers_from_top < self.num_bridged_layers:
                # This layer IS bridged

                # Determine which Sharer layer to use
                s_layer_idx = (self.sharer_num_layers - 1) - layers_from_top

                # Determine which Bridge Block to use (0 is top, N-1 is bottom of bridged stack)
                # Actually, let's index bridge blocks by 'layers_from_top' for simplicity
                # bridge_idx 0 = Topmost layer
                bridge_idx = layers_from_top

                source_hidden = source_hidden_states[s_layer_idx]

                # Pass bridge_idx to forward to pick the correct weights
                proj_key, proj_value = self.forward(source_hidden, target_kv, bridge_idx)

                # MUST clone to break reference chains
                projected_cache.update(proj_key.clone().contiguous(), proj_value.clone().contiguous(), r_layer_idx)

            else:
                # This layer is NOT bridged (too deep / bottom layers)
                # Just copy the original cache
                k, v = target_kv
                projected_cache.update(k.clone().contiguous(), v.clone().contiguous(), r_layer_idx)

        return projected_cache


    def get_gate_stats(self) -> dict:
        """Get current gate statistics for logging."""
        key_gates = [torch.sigmoid(block.gate).item() for block in self.key_modifiers]
        value_gates = [torch.sigmoid(block.gate).item() for block in self.value_modifiers]

        return {
            "key_gates": key_gates,
            "value_gates": value_gates,
            "key_avg": sum(key_gates) / len(key_gates),
            "value_avg": sum(value_gates) / len(value_gates),
        }

"""#Datasets

## Training Dataset
"""

class H2CDatasetWrapper(Dataset):
    def __init__(self, split: str = "train", max_samples: int = None):
        """
        Loads OpenHermes-2.5 and parses it into (prompt, target) pairs.
        """
        print(f"Loading OpenHermes-2.5 ({split})...")
        # Load the dataset from HuggingFace
        self.dataset = load_dataset("teknium/OpenHermes-2.5", split=split)

        if max_samples:
            self.dataset = self.dataset.select(range(max_samples))

        self.data = []
        self._process_data()

    def _process_data(self):
        """
        Iterates through the dataset and extracts the last instruction/response pair.
        """
        valid_count = 0
        for entry in self.dataset:
            convs = entry.get('conversations', [])

            # We look for the last 'human' (or 'user') and 'gpt' (or 'assistant') pair
            first_input = None
            first_target = None

            # Simple logic: Find the first human -> assistant pair (standalone, no context needed)
            for i in range(len(convs) - 1):
                msg = convs[i]
                next_msg = convs[i+1]

                if msg['from'] in ['human', 'user'] and next_msg['from'] in ['gpt', 'assistant']:
                    first_input = msg['value']      # human message -> input (prompt)
                    first_target = next_msg['value'] # assistant message -> target (response)
                    break

            if first_input and first_target:
                self.data.append({
                    "prompt": first_input,
                    "target": first_target
                })
                valid_count += 1

        print(f"Processed {valid_count} valid conversation pairs.")

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        # We return raw text here. Tokenization happens in the Collator.
        # This allows for dynamic padding (much faster training).
        return self.data[idx]

"""## Evaluation Dataset (MMLU)"""

# Full list of MMLU subjects
MMLU_SUBJECTS = [
    'abstract_algebra', 'anatomy', 'astronomy', 'business_ethics', 'clinical_knowledge',
    'college_biology', 'college_chemistry', 'college_computer_science', 'college_mathematics',
    'college_medicine', 'college_physics', 'computer_security', 'conceptual_physics',
    'econometrics', 'electrical_engineering', 'elementary_mathematics', 'formal_logic',
    'global_facts', 'high_school_biology', 'high_school_chemistry', 'high_school_computer_science',
    'high_school_european_history', 'high_school_geography', 'high_school_government_and_politics',
    'high_school_macroeconomics', 'high_school_mathematics', 'high_school_microeconomics',
    'high_school_physics', 'high_school_psychology', 'high_school_statistics', 'high_school_us_history',
    'high_school_world_history', 'human_aging', 'human_sexuality', 'international_law',
    'jurisprudence', 'logical_fallacies', 'machine_learning', 'management', 'marketing',
    'medical_genetics', 'miscellaneous', 'moral_disputes', 'moral_scenarios', 'nutrition',
    'philosophy', 'prehistory', 'professional_accounting', 'professional_law', 'professional_medicine',
    'professional_psychology', 'public_relations', 'security_studies', 'sociology',
    'us_foreign_policy', 'virology', 'world_religions'
]

class MMLUDataset(Dataset):
    def __init__(self, split: str = "auxiliary_train", max_samples: int = None):
        """
        Loads MMLU (cais/mmlu) and parses it into (prompt, target) pairs.
        Used for both Auxiliary Training (split='auxiliary_train') and Validation (split='validation').
        """
        print(f"--- [MMLU] Loading {split} split...")
        # 'all' loads all subjects
        self.dataset = load_dataset("cais/mmlu", "all", split=split)

        if max_samples:
            self.dataset = self.dataset.select(range(max_samples))

        self.data = []
        self._process_data()
        print(f"--- [MMLU] Processed {len(self.data)} examples ({split}).")

    def _process_data(self):
        for entry in self.dataset:
            q = entry["question"]
            choices = entry["choices"]
            answer_idx = entry["answer"]  # 0..3
            answer_letter = "ABCD"[answer_idx]

            # Build the prompt
            context = "Question: " + q + "\n"
            for i, choice in enumerate(choices):
                context += f"{'ABCD'[i]}) {choice}\n"

            instruction = (
                "\nThink carefully, then provide your answer. "
                "Output your final answer as a single letter (A, B, C, or D) on the last line in the format 'Answer': <letter>\n"
            )

            full_prompt = context + instruction

            # For Eval, we might want to separate context/instruction, but for now
            # we'll stick to the simple (prompt, target) format for consistency.
            # The Collator handles tokenization.

            self.data.append({
                "prompt": full_prompt,
                "target": answer_letter,
                "subject": entry.get("subject", "unknown") # cais/mmlu 'all' config might not have subject column easily accessible in this split, but let's try
            })

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        return self.data[idx]

"""#Dataloader"""

from torch.utils.data import DataLoader, random_split, ConcatDataset

class H2CDataModule:
    def __init__(self, tok_sharer, tok_receiver, config):
        self.tok_sharer = tok_sharer
        self.tok_receiver = tok_receiver
        self.batch_size = config["BATCH_SIZE"]
        self.max_samples = config["MAX_SAMPLES"]
        self.samples_per_subject = config["mmlu_sample_size"]

        self.train_loader = None
        self.val_loader = None


    def setup(self):
        """Prepares datasets and splits."""
        print(f"--- [DataModule] Loading Datasets (Max {self.max_samples})...")

        # 1. Base OpenHermes dataset
        oh_dataset = H2CDatasetWrapper(split="train", max_samples=self.max_samples)

        # 2. MMLU auxiliary train dataset (5% mix)
        #    We want MMLU to be ~5% of the total training data
        #    OpenHermes is max_samples. So MMLU should be max_samples * 0.05
        mmlu_train_size = int(self.max_samples * 0.05)
        mmlu_aux_dataset = MMLUDataset(split="auxiliary_train", max_samples=mmlu_train_size)

        # 3. Combine them for training/validation
        full_dataset = ConcatDataset([oh_dataset, mmlu_aux_dataset])
        print(f"--- [DataModule] Combined Train Source Sizes: "
              f"{len(oh_dataset)} OpenHermes + {len(mmlu_aux_dataset)} MMLU Aux "
              f"= {len(full_dataset)} total")

        # 4. Train/Val split on the combined dataset
        train_size = int(0.99 * len(full_dataset))
        val_size = len(full_dataset) - train_size
        self.train_set, self.val_set = random_split(full_dataset, [train_size, val_size])

        print(f"--- [DataModule] Split: {len(self.train_set)} Train | {len(self.val_set)} Val")

        # 5. Shared collator for train/val (same (prompt, target) interface)
        self.collator = H2CDataCollator(self.tok_sharer, self.tok_receiver)

        # 6. Train / Val loaders
        self.train_loader = DataLoader(
            self.train_set,
            batch_size=self.batch_size,
            shuffle=True,
            collate_fn=self.collator,
        )
        self.val_loader = DataLoader(
            self.val_set,
            batch_size=self.batch_size,
            shuffle=False,
            collate_fn=self.collator,
        )

        # 7. MMLU EVAL (Validation Split)
        print(f"--- [DataModule] Setting up MMLU Eval (Validation Split)...")
        # Use validation split for evaluation
        mmlu_eval_dataset = MMLUDataset(split="validation")

        # Reuse the SAME collator because MMLUDataset now produces (prompt, target) just like OpenHermes
        # We don't need MMLUCollator anymore unless we really want the 'subjects' field separate,
        # but H2CDataCollator handles basic training/eval fine.
        # However, H2CEvaluator expects 'subjects' and 'labels' in a specific way.
        # Let's stick to H2CDataCollator for simplicity, but we might lose 'subject' metadata in the batch unless we update Collator.
        # Actually, H2CDataCollator just grabs 'prompt' and 'target'.
        # If we want detailed eval, we might need to tweak H2CDataCollator to pass through 'subject' if present.

        eval_batch_size = max(1, self.batch_size // 2)
        self.mmlu_loader = DataLoader(
            mmlu_eval_dataset,
            batch_size=eval_batch_size,
            shuffle=False,
            collate_fn=self.collator, # Use standard collator
        )


    def get_mmlu_loader(self):
        if not self.train_loader:
            self.setup()
        return self.mmlu_loader

    def get_loaders(self):
        if not self.train_loader:
            self.setup()
        return self.train_loader, self.val_loader

"""#Model Factory"""

class H2CModelFactory:
    def __init__(self, sharer_id, receiver_id, device="cuda", dtype=torch.bfloat16):
        self.sharer_id = sharer_id
        self.receiver_id = receiver_id
        self.device = device
        self.dtype = dtype

        # Placeholders
        self.sharer = None
        self.receiver = None
        self.tok_sharer = None
        self.tok_receiver = None
        self.bridge = None

    def load_tokenizers(self):
        print("--- [ModelFactory] Loading Tokenizers...")
        self.tok_sharer = AutoTokenizer.from_pretrained(self.sharer_id)
        self.tok_receiver = AutoTokenizer.from_pretrained(self.receiver_id)

        # Use left-padding for decoder-only models (required for correct generation)
        self.tok_sharer.padding_side = 'left'
        self.tok_receiver.padding_side = 'left'

        # Ensure pad token is set (some models don't have one by default)
        if self.tok_sharer.pad_token is None:
            self.tok_sharer.pad_token = self.tok_sharer.eos_token
        if self.tok_receiver.pad_token is None:
            self.tok_receiver.pad_token = self.tok_receiver.eos_token

        return self.tok_sharer, self.tok_receiver

    def load_llms(self):
        """Loads the frozen LLMs with quantization."""
        print("--- [ModelFactory] Loading LLMs (Frozen + Quantized)...")

        # 4-bit quantization config for frozen models
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.bfloat16,
            bnb_4bit_use_double_quant=True,  # nested quantization
        )

        self.sharer = AutoModelForCausalLM.from_pretrained(
            self.sharer_id,
            quantization_config=bnb_config,
            device_map="auto",  # required for quantized models
        )

        self.receiver = AutoModelForCausalLM.from_pretrained(
            self.receiver_id,
            quantization_config=bnb_config,
            device_map="auto",
        )

        # Attach tokenizers for convenience
        self.sharer.tokenizer = self.tok_sharer
        self.receiver.tokenizer = self.tok_receiver

        return self.sharer, self.receiver

    def create_bridge(self):
        """Extracts dims and initializes the H2C Projector."""
        print("--- [ModelFactory] Initializing Bridge...")

        s_dims = self._get_model_dims(self.sharer_id)
        r_dims = self._get_model_dims(self.receiver_id)

        self.bridge = H2CProjector(
            sharer_dim=s_dims["hidden_dim"],
            receiver_head_dim=r_dims["head_dim"],
            receiver_num_heads=r_dims["kv_heads"], # Uses KV heads for GQA compatibility
            num_receiver_layers=r_dims["num_layers"],
            sharer_num_layers=s_dims["num_layers"], # Added
            dtype=self.dtype
        ).to(self.device)
        def count_parameters(model):
            total = sum(p.numel() for p in model.parameters())
            trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)
            print(f"Total parameters: {total:,}")
            print(f"Trainable parameters: {trainable:,}")
            print(f"Size (MB): {total * 4 / 1024 / 1024:.1f} MB (float32)")
            return total, trainable

        # After creating the bridge:
        count_parameters(self.bridge)

        return self.bridge

    def _get_model_dims(self, model_id):
        """Helper to extract dimensions safely."""
        config = AutoConfig.from_pretrained(model_id)
        hidden_dim = getattr(config, "hidden_size", getattr(config, "n_embd", None))
        num_heads = getattr(config, "num_attention_heads", getattr(config, "n_head", None))
        kv_heads = getattr(config, "num_key_value_heads", num_heads)
        num_layers = getattr(config, "num_hidden_layers", getattr(config, "n_layer", None))

        head_dim = getattr(config, "head_dim", None)
        if head_dim is None and hidden_dim and num_heads:
            head_dim = hidden_dim // num_heads

        return {
            "hidden_dim": hidden_dim,
            "num_layers": num_layers,
            "num_heads": num_heads,
            "kv_heads": kv_heads,
            "head_dim": head_dim
        }

class H2CBase:
    """
    Base class containing shared logic for the H2C Bridge.
    Handles the common forward pass: Sharer -> Receiver Cache -> Bridge Project.
    """
    def __init__(self, sharer, receiver, bridge, config, device="cuda"):
        self.sharer = sharer
        self.receiver = receiver
        self.bridge = bridge
        self.device = device
        self.config = config

    def get_bridged_cache(self, sharer_ids, sharer_mask, rec_prompt_ids, rec_prompt_mask=None):
        """
        1. Extract Sharer Hidden States (Frozen).
        2. Extract Receiver Initial Cache (Frozen).
        3. Project/Modify Cache (Trainable).
        """
        # A. Run Frozen Models (No Grad)
        receiver_cache = DynamicCache()
        with torch.no_grad():
            # 1. Sharer
            sharer_out = self.sharer(
                input_ids=sharer_ids,
                attention_mask=sharer_mask,
                output_hidden_states=True,
                return_dict=True
            )
            # Clone ALL hidden states to detach them
            # This allows us to delete sharer_out and free VRAM
            sharer_hidden = tuple(h.clone() for h in sharer_out.hidden_states)
            del sharer_out

            # 2. Receiver (Pre-fill)
            # This populates receiver_cache in-place.
            # We capture the output only to delete it immediately.
            rec_out = self.receiver(
                input_ids=rec_prompt_ids,
                attention_mask=rec_prompt_mask,  # Use mask if provided
                past_key_values=receiver_cache,
                use_cache=True
            )
            del rec_out

        # Force Python GC to reclaim the deleted tensors before allocating more VRAM
        gc.collect()

        # B. Run Bridge (Grads allowed if context permits)
        # This is outside no_grad() so the Trainer can track gradients here.
        modified_cache = self.bridge.cache_project(sharer_hidden, receiver_cache)

        # Cleanup intermediate tensors (modified_cache has its own copies)
        del sharer_hidden

        return modified_cache

"""# Collator"""

class H2CDataCollator:
    def __init__(self, tokenizer_sharer, tokenizer_receiver, max_len=1024):
        self.tok_sharer = tokenizer_sharer
        self.tok_receiver = tokenizer_receiver
        self.max_len = max_len

        # Ensure pad tokens
        if self.tok_sharer.pad_token is None: self.tok_sharer.pad_token = self.tok_sharer.eos_token
        if self.tok_receiver.pad_token is None: self.tok_receiver.pad_token = self.tok_receiver.eos_token

    def _get_ids(self, tokenizer, prompt, add_gen):
        # Return list of ints
        return tokenizer.apply_chat_template(
            [{"role": "user", "content": prompt}],
            tokenize=True,
            add_generation_prompt=add_gen,
            return_dict=False
        )

    def __call__(self, batch):
        prompts = [x['prompt'] for x in batch]
        targets = [x['target'] for x in batch]

        # Lists for padding later
        sharer_ids_list = []
        rec_prompt_ids_list = [] # The prompt MINUS the last token
        rec_kickstart_list = []  # The extracted last token
        rec_target_ids_list = []

        for p, t in zip(prompts, targets):
            # 1. Sharer: Tokenize & Strip Last & Truncate
            s_raw = self._get_ids(self.tok_sharer, p, add_gen=True)
            s_raw = s_raw[:self.max_len]  # Truncate to max_len
            sharer_ids_list.append(s_raw[:-1]) # Strip last

            # 2. Receiver: Tokenize & Strip Last & Truncate
            r_raw = self._get_ids(self.tok_receiver, p, add_gen=True)
            r_raw = r_raw[:self.max_len]  # Truncate to max_len
            rec_prompt_ids_list.append(r_raw[:-1]) # Strip last (The 'Prompt')
            rec_kickstart_list.append([r_raw[-1]]) # Keep last (The 'Kickstart')

            # 3. Target: Tokenize & Truncate
            t_ids = self.tok_receiver(t, add_special_tokens=False)["input_ids"]
            t_ids = t_ids[:self.max_len]  # Truncate to max_len
            if len(t_ids) > 0 and t_ids[-1] != self.tok_receiver.eos_token_id:
                t_ids.append(self.tok_receiver.eos_token_id)
            rec_target_ids_list.append(t_ids)

        # --- Padding ---
        # 1. Sharer Inputs
        sharer_batch = self.tok_sharer.pad(
            {"input_ids": sharer_ids_list}, padding=True,  return_tensors="pt"
        )

        # 2. Receiver Inputs (Stripped Prompt)
        rec_prompt_batch = self.tok_receiver.pad(
            {"input_ids": rec_prompt_ids_list}, padding=True,  return_tensors="pt"
        )

        # 3. Receiver Kickstart (It's just 1 token, but we tensor-ize it)
        rec_kickstart_batch = torch.tensor(rec_kickstart_list)

        # 4. Receiver Targets
        rec_target_batch = self.tok_receiver.pad(
            {"input_ids": rec_target_ids_list}, padding=True, return_tensors="pt"
        )

        # 5. Metadata (Subjects & Labels for Eval)
        subjects = [x.get('subject', 'unknown') for x in batch]
        # 'target' in batch is the raw string (e.g. "A"). Use that as label.
        labels = [x.get('target', 'unknown') for x in batch]
        # 6. Raw text for text-to-text baseline
        raw_contexts = []
        raw_instructions = []
        for p in prompts:
          split_marker = "\Think carefully"
          if split_marker in p:
              context, instruction = p.rsplit(split_marker, 1)
              raw_contexts.append(context)
              raw_instructions.append(split_marker + instruction)
          else:
              raw_contexts.append(p)
              raw_instructions.append("")

        return {
            "sharer_input_ids": sharer_batch['input_ids'],
            "sharer_mask": sharer_batch['attention_mask'],

            "receiver_prompt_ids": rec_prompt_batch['input_ids'],     # The prefill input
            "receiver_prompt_mask": rec_prompt_batch['attention_mask'], # Mask for prefill
            "receiver_kickstart_ids": rec_kickstart_batch,            # The \n token

            "receiver_target_ids": rec_target_batch['input_ids'],
            "receiver_target_mask": rec_target_batch['attention_mask'], # Mask for targets

            "raw_context": raw_contexts,
            "raw_instruction": raw_instructions,

            "subjects": subjects,
            "labels": labels
        }

"""#Evaluation

## OpenHermes Eval
"""

class H2CEvaluator(H2CBase):
    def __init__(self, sharer, receiver, bridge, tok_receiver, config, device="cuda"):
        super().__init__(sharer, receiver, bridge, config, device)
        self.tok_receiver = tok_receiver

    @torch.no_grad()
    def evaluate_loss(self, dataloader):
        self.bridge.eval()
        total_loss = 0
        num_batches = 0

        progress_bar = tqdm(dataloader, desc="Validation Loop", leave=False)

        for batch in progress_bar:
            # 1. Unpack
            sharer_ids = batch['sharer_input_ids'].to(self.device)
            sharer_mask = batch['sharer_mask'].to(self.device)
            rec_prompt_ids = batch['receiver_prompt_ids'].to(self.device)
            rec_prompt_mask = batch['receiver_prompt_mask'].to(self.device)
            rec_kickstart_ids = batch['receiver_kickstart_ids'].to(self.device)
            rec_target_ids = batch['receiver_target_ids'].to(self.device)
            rec_target_mask = batch['receiver_target_mask'].to(self.device)

            # Use autocast to match training dtype
            with torch.autocast(device_type="cuda", dtype=torch.bfloat16):
                modified_cache = self.get_bridged_cache(sharer_ids, sharer_mask, rec_prompt_ids, rec_prompt_mask)
                del sharer_ids, sharer_mask, rec_prompt_ids, rec_prompt_mask

                combined_input = torch.cat([rec_kickstart_ids, rec_target_ids], dim=1)
                combined_mask = torch.cat([torch.ones_like(rec_kickstart_ids), rec_target_mask], dim=1)

                # Shape Assertions
                assert modified_cache is not None
                assert len(modified_cache) > 0

                # Build full attention mask including cache positions
                cache_len = modified_cache[0][0].shape[2]
                batch_size = combined_input.shape[0]

                # Assertions
                assert combined_input.shape[0] == batch_size
                assert combined_mask.shape[0] == batch_size

                cache_mask = torch.ones(batch_size, cache_len, device=self.device, dtype=combined_mask.dtype)
                full_attention_mask = torch.cat([cache_mask, combined_mask], dim=1)

                # Create labels: -100 for kickstart (ignore), actual tokens for targets
                # Build target labels with padding masked first
                target_labels = rec_target_ids.clone()
                target_labels[rec_target_mask == 0] = -100  # Mask padding in targets

                # Concatenate kickstart (-100) with masked target labels
                labels = torch.cat([
                    torch.full_like(rec_kickstart_ids, -100),  # Ignore kickstart in loss
                    target_labels  # Target tokens with padding already masked
                ], dim=1)

                del rec_kickstart_ids, rec_target_ids, rec_target_mask

                outputs = self.receiver(
                    input_ids=combined_input,
                    past_key_values=modified_cache,
                    attention_mask=full_attention_mask,
                    labels=labels
                )
                del combined_input, combined_mask, full_attention_mask, labels, modified_cache

            total_loss += outputs.loss.item()
            del outputs
            num_batches += 1

        # Cleanup after eval loop
        torch.cuda.empty_cache()

        avg_loss = total_loss / num_batches if num_batches > 0 else 0
        self.bridge.train()
        return avg_loss

    @torch.no_grad()
    def generate_demo(self, prompt_text, max_new_tokens=500):
        self.bridge.eval()
        print(f"\nPROMPT: {prompt_text}\n" + "-"*20)

        # --- A. Manual Tokenization ---
        # Sharer
        s_raw = self.sharer.tokenizer.apply_chat_template(
            [{"role": "user", "content": prompt_text}],
            tokenize=True, add_generation_prompt=True, return_dict=False
        )
        s_ids = torch.tensor([s_raw[:-1]]).to(self.device) # Strip Last
        s_mask = torch.ones_like(s_ids).to(self.device)    # Mask is all 1s

        # Receiver
        r_raw = self.tok_receiver.apply_chat_template(
            [{"role": "user", "content": prompt_text}],
            tokenize=True, add_generation_prompt=True, return_dict=False
        )

        r_input_full = torch.tensor([r_raw]).to(self.device)           # For Vanilla
        r_input_stripped = torch.tensor([r_raw[:-1]]).to(self.device)  # For Bridge (Context)
        r_kickstart = torch.tensor([r_raw[-1:]]).to(self.device)       # For Bridge (Input)

        # --- B. Vanilla Control Group ---
        vanilla_out = self.receiver.generate(
            r_input_full,
            max_new_tokens=max_new_tokens, do_sample=False,
            pad_token_id=self.tok_receiver.pad_token_id
        )
        v_text = self.tok_receiver.decode(vanilla_out[0], skip_special_tokens=False)
        print(f"[Vanilla]:\n{v_text.replace(prompt_text, '').strip()}")

        # --- C. Bridged Generation ---
        # 1. Use Helper Function (with autocast to match training dtype)
        r_mask = torch.ones_like(r_input_stripped).to(self.device)  # No padding in single sample
        with torch.autocast(device_type="cuda", dtype=torch.bfloat16):
            modified_cache = self.get_bridged_cache(s_ids, s_mask, r_input_stripped, r_mask)

            # 2. Generate
            bridged_out = self.receiver.generate(
                input_ids=r_input_full,  # Pass FULL input (not just kickstart)
                past_key_values=modified_cache,
                max_new_tokens=max_new_tokens,
                do_sample=False,
                pad_token_id=self.tok_receiver.pad_token_id
            )

        b_text = self.tok_receiver.decode(bridged_out[0], skip_special_tokens=False)
        print(f"\n[Bridged]:\n{b_text.strip()}")
        print("-" * 20 + "\n")

        # Cleanup
        del s_ids, s_mask, r_input_full, r_input_stripped, r_kickstart, r_mask
        del vanilla_out, modified_cache, bridged_out
        torch.cuda.empty_cache()

        self.bridge.train()

"""## MMLU Eval"""

class H2CMMLUEvaluator(H2CBase):
    def __init__(self, sharer, receiver, bridge, tok_receiver, tok_sharer, config, device="cuda"):
        super().__init__(sharer, receiver, bridge, config, device)
        self.tok_receiver = tok_receiver
        self.tok_sharer = tok_sharer

    @torch.no_grad()
    def evaluate_baselines(self, dataloader, max_new_tokens=15):
        """Runs the 3 static baselines and returns a dict of results."""
        print("\n" + "="*30)
        print(">>> RUNNING BASELINES (Deterministic)")
        print("="*30 + "\n")

        results = {}
        modes = ["receiver_only", "sharer_only", "text_to_text"]

        for mode in modes:
            print(f"--- Running Baseline: {mode} ---")
            acc, err, lat = self._eval_loop(dataloader, mode=mode, max_new_tokens=max_new_tokens)
            results[mode] = {"acc": acc, "err": err, "latency_ms": lat}

        return results

    @torch.no_grad()
    def evaluate_baselines_detailed(self, dataloader, max_new_tokens=15, include_text_to_text=True):
        """
        Runs all baselines with detailed results for visualization comparison.
        Returns dict mapping mode -> {acc, err, latency_ms, details}

        Args:
            include_text_to_text: If True, includes text-to-text baseline (slower but complete)
        """
        print("\n" + "="*30)
        print(">>> RUNNING BASELINES (Detailed)")
        print("="*30 + "\n")

        results = {}
        modes = ["receiver_only", "sharer_only"]
        if include_text_to_text:
            modes.append("text_to_text")

        for mode in modes:
            print(f"--- Running Baseline (Detailed): {mode} ---")
            acc, err, lat, details = self._eval_loop(
                dataloader, mode=mode, max_new_tokens=max_new_tokens, collect_details=True
            )
            results[mode] = {
                "acc": acc,
                "err": err,
                "latency_ms": lat,
                "details": details
            }

        return results

    @torch.no_grad()
    def evaluate_accuracy(self, dataloader, max_new_tokens=15):
        """Standard evaluation for the Trainable Bridge."""
        self.bridge.eval()
        return self._eval_loop(dataloader, mode="bridge", max_new_tokens=max_new_tokens)

    @torch.no_grad()
    def evaluate_accuracy_detailed(self, dataloader, max_new_tokens=15):
        """
        Detailed evaluation that returns predictions, labels, and subjects.
        Used for confusion matrix and per-category analysis.
        Returns: (accuracy, error_rate, latency, detailed_results)
        where detailed_results is a list of dicts with keys: pred, label, subject, correct
        """
        self.bridge.eval()
        return self._eval_loop(dataloader, mode="bridge", max_new_tokens=max_new_tokens, collect_details=True)

    def _eval_loop(self, dataloader, mode, max_new_tokens, collect_details=False):
        stats = {"correct": 0, "total": 0, "format_errors": 0, "total_time": 0.0}
        detailed_results = [] if collect_details else None

        progress_bar = tqdm(dataloader, desc=f"Eval [{mode}]", leave=False, mininterval=3.0)

        for batch_idx, batch in enumerate(progress_bar):
            # 1. Measure Generation Time

            start_time = time.time()
            prompt_texts, gen_texts, labels = self._generate_batch(batch, mode, max_new_tokens)
            end_time = time.time()

            # Get subjects if available (for detailed analysis)
            subjects = batch.get('subjects', ['unknown'] * len(labels))

            # Update Time Stats
            stats["total_time"] += (end_time - start_time)

            # 2. Score & Log
            batch_details = self._score_batch(prompt_texts, gen_texts, labels, stats, subjects, collect_details)
            if collect_details and batch_details:
                detailed_results.extend(batch_details)

            # Periodic memory cleanup (every 10 batches)
            if batch_idx % 10 == 0:
                torch.cuda.empty_cache()

        # Calculate Metrics
        total = stats["total"] if stats["total"] > 0 else 1
        accuracy = stats["correct"] / total
        error_rate = stats["format_errors"] / total
        avg_latency_ms = (stats["total_time"] / total)

        print(f"[{mode}] Acc: {accuracy:.2%} | Err: {error_rate:.2%} | Latency: {avg_latency_ms:.1f}ms")

        if mode == "bridge": self.bridge.train()

        if collect_details:
            return accuracy, error_rate, avg_latency_ms, detailed_results
        return accuracy, error_rate, avg_latency_ms

    @torch.no_grad()
    def evaluate_baselines_detailed(self, dataloader, max_new_tokens=15, include_text_to_text=True):
        """
        Runs all baselines with detailed results for visualization comparison.
        Returns dict mapping mode -> (acc, err, lat, detailed_results)

        Args:
            include_text_to_text: If True, includes text-to-text baseline (slower but complete)
        """
        print("\n" + "="*30)
        print(">>> RUNNING BASELINES (Detailed)")
        print("="*30 + "\n")

        results = {}
        modes = ["receiver_only", "sharer_only"]
        if include_text_to_text:
            modes.append("text_to_text")

        for mode in modes:
            print(f"--- Running Baseline: {mode} ---")
            acc, err, lat, details = self._eval_loop(
                dataloader, mode=mode, max_new_tokens=max_new_tokens, collect_details=True
            )
            results[mode] = {
                "acc": acc, "err": err, "latency_ms": lat, "details": details
            }

        return results


    def _generate_batch(self, batch, mode, max_new_tokens):
        # Standard Tensors
        sharer_ids = batch['sharer_input_ids'].to(self.device)
        sharer_mask = batch['sharer_mask'].to(self.device)
        rec_full_ids = batch['receiver_prompt_ids'].to(self.device)
        rec_mask = batch['receiver_prompt_mask'].to(self.device)

        prompt_texts = []
        gen_texts = []

        if mode == "bridge":
            # Bridge Logic (Standard) - use autocast to match training dtype
            with torch.autocast(device_type="cuda", dtype=torch.bfloat16):
                # Create cache from all but last token
                rec_context = rec_full_ids[:, :-1]
                rec_context_mask = rec_mask[:, :-1]  # Slice mask to match context
                modified_cache = self.get_bridged_cache(sharer_ids, sharer_mask, rec_context, rec_context_mask)
                del rec_context, rec_context_mask


                outputs = self.receiver.generate(
                    input_ids=rec_full_ids,  # Pass FULL input (model handles cache internally)
                    past_key_values=modified_cache,
                    attention_mask=rec_mask,
                    max_new_tokens=max_new_tokens,
                    pad_token_id=self.tok_receiver.pad_token_id,
                    do_sample=False
                )
                del modified_cache
            prompt_texts = self.tok_receiver.batch_decode(rec_full_ids, skip_special_tokens=False)
            gen_texts = self.tok_receiver.batch_decode(outputs[:, rec_full_ids.shape[1]:], skip_special_tokens=False)
            del outputs

        elif mode == "receiver_only":
            # Reconstruct full input (Prompt + Kickstart)
            rec_kickstart = batch['receiver_kickstart_ids'].to(self.device)
            full_input = torch.cat([rec_full_ids, rec_kickstart], dim=1)
            full_mask = torch.cat([rec_mask, torch.ones_like(rec_kickstart)], dim=1)

            outputs = self.receiver.generate(
                input_ids=full_input, attention_mask=full_mask,
                max_new_tokens=max_new_tokens, pad_token_id=self.tok_receiver.pad_token_id, do_sample=False
            )
            prompt_texts = self.tok_receiver.batch_decode(full_input, skip_special_tokens=False)
            gen_texts = self.tok_receiver.batch_decode(outputs[:, full_input.shape[1]:], skip_special_tokens=False)
            del outputs, full_input, full_mask

        elif mode == "sharer_only":
            # Sharer input is already full (padded) in the batch
            outputs = self.sharer.generate(
                input_ids=sharer_ids, attention_mask=sharer_mask,
                max_new_tokens=max_new_tokens, pad_token_id=self.tok_sharer.pad_token_id, do_sample=False
            )
            prompt_texts = self.tok_sharer.batch_decode(sharer_ids, skip_special_tokens=False)
            gen_texts = self.tok_sharer.batch_decode(outputs[:, sharer_ids.shape[1]:], skip_special_tokens=False)
            del outputs

        elif mode == "text_to_text":
            # --- FIXED T2T LOGIC ---

            # 1. Get Raw Components
            raw_contexts = batch['raw_context']
            raw_instructions = batch['raw_instruction']

            # 2. Sharer Step (Generate Hint)
            # We wrap this in a chat template so the Sharer knows it's a request
            sharer_inputs_formatted = []
            for ctx in raw_contexts:
                s_prompt = f"{ctx}\n\nIn one sentence, give a hint on how to solve the problem. Do NOT explicitly say the answer. Focus on the key logic and approach needed."
                sharer_inputs_formatted.append([{"role": "user", "content": s_prompt}])

            s_encoded = self.tok_sharer.apply_chat_template(
                sharer_inputs_formatted,
                tokenize=True,
                add_generation_prompt=True,
                padding=True,
                return_tensors="pt",
                return_dict=True
            )
            s_inputs = s_encoded["input_ids"].to(self.device)
            s_attn_mask = s_encoded["attention_mask"].to(self.device)

            s_out = self.sharer.generate(
                s_inputs,
                attention_mask=s_attn_mask,
                max_new_tokens=48,
                pad_token_id=self.tok_sharer.pad_token_id,
                do_sample=False
            )

            # Decode only the new tokens (the hint)
            s_hints = self.tok_sharer.batch_decode(
                s_out[:, s_inputs.shape[1]:],
                skip_special_tokens=False
            )

            # 3. Receiver Step (Context + Hint + Instruction)
            receiver_inputs_formatted = []
            for ctx, hint, instr in zip(raw_contexts, s_hints, raw_instructions):
                # Clean the hint to prevent formatting issues
                clean_hint = hint.replace("\n", " ").strip()

                # Construct the final prompt
                # Note: We put the hint INSIDE the user message
                final_content = f"{ctx}\n\n[Hint: {clean_hint}]\n\n{instr}"
                receiver_inputs_formatted.append([{"role": "user", "content": final_content}])

            # CRITICAL FIX: Use apply_chat_template for Receiver too
            r_encoded = self.tok_receiver.apply_chat_template(
                receiver_inputs_formatted,
                tokenize=True,
                add_generation_prompt=True,  # Adds <|im_start|>assistant...
                padding=True,
                return_tensors="pt",
                return_dict=True
            )
            r_inputs = r_encoded["input_ids"].to(self.device)
            r_attn_mask = r_encoded["attention_mask"].to(self.device)

            r_out = self.receiver.generate(
                r_inputs,
                attention_mask=r_attn_mask,
                max_new_tokens=max_new_tokens,
                pad_token_id=self.tok_receiver.pad_token_id,
                do_sample=False
            )

            # Store prompts for logging (we use the raw text version for readability)
            prompt_texts = [m[0]['content'] for m in receiver_inputs_formatted]
            gen_texts = self.tok_receiver.batch_decode(
                r_out[:, r_inputs.shape[1]:],
                skip_special_tokens=False
            )
            # Cleanup T2T tensors
            del s_inputs, s_attn_mask, s_out, r_inputs, r_attn_mask, r_out

        # Cleanup - tensors created at function start
        del sharer_ids, sharer_mask, rec_full_ids, rec_mask

        return prompt_texts, gen_texts, batch['labels']

    def _score_batch(self, prompt_texts, gen_texts, labels, stats, subjects=None, collect_details=False):
        """Scores the batch and prints verbose logs if enabled."""
        if subjects is None:
            subjects = ['unknown'] * len(labels)

        batch_details = [] if collect_details else None

        for prompt, gen_text, truth, subject in zip(prompt_texts, gen_texts, labels, subjects):
            pred = self._extract_json_answer(gen_text)

            is_correct = False
            is_invalid = False

            if pred == "INVALID":
                stats["format_errors"] += 1
                is_invalid = True
            elif pred == truth:
                stats["correct"] += 1
                is_correct = True

            stats["total"] += 1

            # Collect detailed results if requested
            if collect_details:
                batch_details.append({
                    "pred": pred,
                    "label": truth,
                    "subject": subject,
                    "correct": is_correct,
                    "invalid": is_invalid
                })

            # --- VERBOSE LOGGING ---
            if self.config.get("verbose", False):
                self._log_sample(prompt, gen_text, truth, pred, is_correct, is_invalid)

        return batch_details

    def _log_sample(self, prompt, gen_text, truth, pred, is_correct, is_invalid):
        if is_invalid:
            status = "âš ï¸ FORMAT ERROR"
        else:
            status = "âœ… CORRECT" if is_correct else "âŒ WRONG"

        print("\n" + "="*50)
        # Only show last 300 chars of prompt to keep log clean
        print(f"--- PROMPT ---\n...{prompt.strip()[-300:]}")
        print(f"--- RESPONSE ---\n{gen_text.strip()}")
        print(f"--- EVAL ---")
        print(f"Truth: {truth} | Pred: {pred} | Result: {status}")
        print("="*50 + "\n")

    def _extract_json_answer(self, text):
        """Robust Extractor - handles many answer formats"""
        text = text.strip()

        # "answer: A", "answer is A", "answer A", etc.
        match = re.search(r'answer[\s:\"\']*([A-D])', text, re.IGNORECASE)
        if match: return match.group(1).upper()

        # "the answer is A"
        match = re.search(r'answer\s*is\s*([A-D])', text, re.IGNORECASE)
        if match: return match.group(1).upper()

        # "option A", "option A)", "option: A"
        match = re.search(r'option[\s:]*([A-D])', text, re.IGNORECASE)
        if match: return match.group(1).upper()

        # Starts with "A)" or "A." or "A " followed by text (e.g., "C) Hurler syndrome")
        match = re.match(r'^([A-D])[\)\.\s]', text, re.IGNORECASE)
        if match: return match.group(1).upper()

        # Just "(A)" or "A" alone with optional parens/whitespace
        match = re.match(r'^[\(\s]*([A-D])[\)\s]*$', text, re.IGNORECASE)
        if match: return match.group(1).upper()

        # Repeated letter like "C C" or "A A"
        match = re.match(r'^([A-D])\s+\1', text, re.IGNORECASE)
        if match: return match.group(1).upper()

        # Last resort: find the first standalone A/B/C/D in the text
        match = re.search(r'\b([A-D])\b', text)
        if match: return match.group(1).upper()

        return "INVALID"

"""#Trainer"""

class H2CTrainer(H2CBase):
    def __init__(self, sharer, receiver, bridge, optimizer, config, device="cuda"):
        super().__init__(sharer, receiver, bridge, config, device)
        self.optimizer = optimizer

        # Freeze Models
        self.sharer.eval()
        self.sharer.requires_grad_(False)
        self.receiver.eval()
        self.receiver.requires_grad_(False)

        # Bridge is Trainable
        self.bridge.train()
        self.bridge.requires_grad_(True)

    def train_step(self, batch):
        # 1. Unpack Batch
        sharer_ids = batch['sharer_input_ids'].to(self.device)
        sharer_mask = batch['sharer_mask'].to(self.device)
        rec_prompt_ids = batch['receiver_prompt_ids'].to(self.device)
        rec_prompt_mask = batch['receiver_prompt_mask'].to(self.device)
        rec_kickstart_ids = batch['receiver_kickstart_ids'].to(self.device)
        rec_target_ids = batch['receiver_target_ids'].to(self.device)
        rec_target_mask = batch['receiver_target_mask'].to(self.device)

        # 2. Get Modified Cache (sharer/receiver inputs only needed here)
        with torch.autocast(device_type="cuda", dtype=torch.bfloat16):
            modified_cache = self.get_bridged_cache(sharer_ids, sharer_mask, rec_prompt_ids, rec_prompt_mask)

        # Free sharer inputs immediately after cache creation
        del sharer_ids, sharer_mask, rec_prompt_ids, rec_prompt_mask

        # 3. Calculate Loss
        with torch.autocast(device_type="cuda", dtype=torch.bfloat16):
            combined_input = torch.cat([rec_kickstart_ids, rec_target_ids], dim=1)
            # Build attention mask: kickstart is always attended, target uses its mask
            combined_mask = torch.cat([torch.ones_like(rec_kickstart_ids), rec_target_mask], dim=1)

            # Build full attention mask including cache positions
            cache_len = modified_cache[0][0].shape[2]
            batch_size = combined_input.shape[0]
            cache_mask = torch.ones(batch_size, cache_len, device=self.device, dtype=combined_mask.dtype)
            full_attention_mask = torch.cat([cache_mask, combined_mask], dim=1)

            # Create labels: -100 for kickstart (ignore), actual tokens for targets
            # Build target labels with padding masked first
            target_labels = rec_target_ids.clone()
            target_labels[rec_target_mask == 0] = -100  # Mask padding in targets

            # Concatenate kickstart (-100) with masked target labels
            labels = torch.cat([
                torch.full_like(rec_kickstart_ids, -100),  # Ignore kickstart in loss
                target_labels  # Target tokens with padding already masked
            ], dim=1)

            del rec_kickstart_ids, rec_target_ids, rec_target_mask  # No longer needed

            outputs = self.receiver(
                input_ids=combined_input,
                past_key_values=modified_cache,
                attention_mask=full_attention_mask,
                labels=labels,
            )
            del combined_input, combined_mask, labels  # No longer needed

            loss = outputs.loss

        # 4. Optimize
        self.optimizer.zero_grad()
        loss.backward()

        # Free computation graph after backward (gradients computed)
        del modified_cache, outputs

        # Collect gradient norms for diagnostics
        key_attn_grad = 0.0
        value_attn_grad = 0.0
        gate_grad = 0.0
        for block in self.bridge.key_modifiers:
            if block.attn.out_proj.weight.grad is not None:
                key_attn_grad += block.attn.out_proj.weight.grad.norm().item()
            if block.gate.grad is not None:
                gate_grad += block.gate.grad.norm().item()
        for block in self.bridge.value_modifiers:
            if block.attn.out_proj.weight.grad is not None:
                value_attn_grad += block.attn.out_proj.weight.grad.norm().item()
            if block.gate.grad is not None:
                gate_grad += block.gate.grad.norm().item()

        grad_norm = torch.nn.utils.clip_grad_norm_(self.bridge.parameters(), 1.0)

        # Log metrics (need .item() before deleting)
        loss_value = loss.item()
        del loss

        wandb.log({
            "Training/Grad Norm": grad_norm.item(),
            "Training/Key Attn Grad Norm": key_attn_grad,
            "Training/Value Attn Grad Norm": value_attn_grad,
            "Training/Gate Grad Norm": gate_grad,
        })

        self.optimizer.step()

        return loss_value

"""# Model Engine"""

class H2CEngine:
    def __init__(self, factory, data_module, config, lr=1e-4, eval_every=50, checkpoint_path=None, wandb_run_id=None):
        self.factory = factory
        self.dm = data_module
        self.config = config
        self.lr = lr
        self.eval_every = eval_every
        self.global_step = 0
        self.best_accuracy = 0.0  # Track best accuracy for "save best" logic

        # Create checkpoint naming based on model names
        sharer_name = config['SHARER_ID'].split('/')[-1].replace('.', '-')
        receiver_name = config['RECEIVER_ID'].split('/')[-1].replace('.', '-')
        self.checkpoint_prefix = f"bridge_{sharer_name}_TO_{receiver_name}"

        # Store wandb run ID for resuming
        self.wandb_run_id = wandb_run_id

        # Component Setup
        self._setup_wandb()
        self._setup_models()
        self._setup_optimization()
        self._setup_evaluators()

        # Load from checkpoint if provided
        if checkpoint_path:
            self.load_checkpoint(checkpoint_path)

    def _setup_wandb(self):
        run_name = f"{self.config['SHARER_ID'].split('/')[-1]}_TO_{self.config['RECEIVER_ID'].split('/')[-1]}"

        if self.wandb_run_id:
            # Resume existing run
            self.wandb_run = wandb.init(
                id=self.wandb_run_id,
                project="nlp_project",
                resume="must",  # Will error if run doesn't exist
                reinit=True
            )
            print(f"Resumed wandb run: {self.wandb_run_id}")
        else:
            # Start new run
            self.wandb_run = wandb.init(
                name=run_name.replace("\\", "-"),
                project="nlp_project",
                config=self.config,
                save_code=True,
                reinit=True
            )


    def _setup_models(self):
        self.sharer, self.receiver = self.factory.load_llms()
        self.bridge = self.factory.create_bridge()


    def _setup_optimization(self):
            self.train_loader, self.val_loader = self.dm.get_loaders()
            self.mmlu_loader = self.dm.get_mmlu_loader()
            self.optimizer = optim.AdamW(self.bridge.parameters(), lr=self.lr)

            # Setup cosine scheduler with warmup
            # Calculate total training steps
            epochs = self.config.get('epochs', 1)  # Default to 1 if not specified
            total_steps = len(self.train_loader) * epochs
            warmup_steps = int(0.1 * total_steps)  # 10% warmup

            self.scheduler = get_cosine_schedule_with_warmup(
                self.optimizer,
                num_warmup_steps=warmup_steps,
                num_training_steps=total_steps
            )

            print(f"--- [Scheduler] Cosine schedule: {total_steps} total steps, {warmup_steps} warmup steps")

            self.trainer = H2CTrainer(
                self.sharer, self.receiver, self.bridge, self.optimizer, self.config, device=self.factory.device
            )
    def _setup_evaluators(self):
        self.evaluator = H2CEvaluator(
            self.sharer, self.receiver, self.bridge, self.factory.tok_receiver, self.config, device=self.factory.device
        )
        self.mmlu_evaluator = H2CMMLUEvaluator(
            self.sharer, self.receiver, self.bridge, self.factory.tok_receiver, self.factory.tok_sharer, self.config, device=self.factory.device
        )

    def run(self, epochs=3):
        print(f"--- [Engine] Starting Training for {epochs} epochs...")
        for epoch in range(epochs):
            self._run_epoch(epoch)

        # Save final checkpoint at end of training
        print("\n--- Training Complete ---")
        self._save_checkpoint(checkpoint_type="final")
        print(f"Best accuracy achieved: {self.best_accuracy:.2%}")

    def _run_epoch(self, epoch_idx):
        self.bridge.train()
        progress_bar = tqdm(self.train_loader, desc=f"Epoch {epoch_idx + 1}")
        log_bridge_every = self.config.get("log_bridge_every", 100)

        for batch in progress_bar:


            loss = self.trainer.train_step(batch)
            self.global_step += 1

            # Step the scheduler (if it exists)
            self.scheduler.step()

            # Get current learning rate for logging
            current_lr = self.optimizer.param_groups[0]['lr']

            progress_bar.set_postfix({"loss": f"{loss:.4f}", "lr": f"{current_lr:.2e}"})
            wandb.log({
                "Training/Loss": loss,
                "Training/Learning Rate": current_lr,
                "epoch": epoch_idx + 1
            })

            # Log bridge stats periodically (without printing)
            if self.global_step % log_bridge_every == 0:
                self._log_bridge_stats(print_output=False)

            if self.global_step % self.eval_every == 0:
                self._perform_eval(progress_bar)


    def _log_bridge_stats(self, print_output=True):
        """Log gate values and bridge statistics."""
        stats = self.bridge.get_gate_stats()

        key_gates = stats["key_gates"]
        value_gates = stats["value_gates"]
        avg_key_gate = stats["key_avg"]
        avg_value_gate = stats["value_avg"]


        # Count how many gates are "on" (>0.5) vs "off" (<0.5)
        key_on = sum(1 for g in key_gates if g > 0.5)
        value_on = sum(1 for g in value_gates if g > 0.5)

        # Get raw gate values (before clamping)
        key_raw_gates = [block.gate.item() for block in self.bridge.key_modifiers]
        value_raw_gates = [block.gate.item() for block in self.bridge.value_modifiers]
        avg_key_raw = sum(key_raw_gates) / len(key_raw_gates)
        avg_value_raw = sum(value_raw_gates) / len(value_raw_gates)

        # Helper to compute mean/std safely
        def _mean(values):
            return sum(values) / len(values) if values else 0.0

        def _std(values, mean):
            return (sum((v - mean) ** 2 for v in values) / len(values)) ** 0.5 if values else 0.0

        # Helper to extract q/k/v norms from MultiheadAttention
        def _qkv_norms(attn_module):
            q_norm = k_norm = v_norm = 0.0

            # Case 1: Standard Self-Attention (one fused matrix)
            if hasattr(attn_module, "in_proj_weight") and attn_module.in_proj_weight is not None:
                with torch.no_grad():
                    q, k, v = attn_module.in_proj_weight.detach().chunk(3, dim=0)
                    q_norm = q.norm().item()
                    k_norm = k.norm().item()
                    v_norm = v.norm().item()

            # Case 2: Cross-Attention (separate matrices, kdim/vdim used)
            # PyTorch's MultiheadAttention stores them as q_proj_weight, k_proj_weight, v_proj_weight
            else:
                if hasattr(attn_module, "q_proj_weight") and attn_module.q_proj_weight is not None:
                    q_norm = attn_module.q_proj_weight.detach().norm().item()
                if hasattr(attn_module, "k_proj_weight") and attn_module.k_proj_weight is not None:
                    k_norm = attn_module.k_proj_weight.detach().norm().item()
                if hasattr(attn_module, "v_proj_weight") and attn_module.v_proj_weight is not None:
                    v_norm = attn_module.v_proj_weight.detach().norm().item()

            return q_norm, k_norm, v_norm

        # Get attention weight norms (is attention learning?)
        key_attn_norms = [block.attn.out_proj.weight.norm().item() for block in self.bridge.key_modifiers]
        value_attn_norms = [block.attn.out_proj.weight.norm().item() for block in self.bridge.value_modifiers]
        avg_key_attn_norm = _mean(key_attn_norms)
        avg_value_attn_norm = _mean(value_attn_norms)
        std_key_attn_norm = _std(key_attn_norms, avg_key_attn_norm)
        std_value_attn_norm = _std(value_attn_norms, avg_value_attn_norm)

        # Q/K/V norms for attention input projections
        key_q_norms, key_k_norms, key_v_norms = [], [], []
        value_q_norms, value_k_norms, value_v_norms = [], [], []
        for block in self.bridge.key_modifiers:
            qn, kn, vn = _qkv_norms(block.attn)
            key_q_norms.append(qn)
            key_k_norms.append(kn)
            key_v_norms.append(vn)
        for block in self.bridge.value_modifiers:
            qn, kn, vn = _qkv_norms(block.attn)
            value_q_norms.append(qn)
            value_k_norms.append(kn)
            value_v_norms.append(vn)

        avg_key_q_norm = _mean(key_q_norms)
        avg_key_k_norm = _mean(key_k_norms)
        avg_key_v_norm = _mean(key_v_norms)
        avg_value_q_norm = _mean(value_q_norms)
        avg_value_k_norm = _mean(value_k_norms)
        avg_value_v_norm = _mean(value_v_norms)

        if print_output:
            print(f"Bridge Gates: Key avg={avg_key_gate:.3f} ({key_on}/{len(key_gates)} on) | "
                  f"Value avg={avg_value_gate:.3f} ({value_on}/{len(value_gates)} on)")
            print(f"  raw gates:      Key={avg_key_raw:.4f} | Value={avg_value_raw:.4f}")
            print(f"  attn norms:     Key={avg_key_attn_norm:.4f} | Value={avg_value_attn_norm:.4f}")
            print(f"  qkv norms (Key): Q={avg_key_q_norm:.4f} K={avg_key_k_norm:.4f} V={avg_key_v_norm:.4f}")
            print(f"  qkv norms (Val): Q={avg_value_q_norm:.4f} K={avg_value_k_norm:.4f} V={avg_value_v_norm:.4f}")

        # Log to wandb
        wandb.log({
            # Gate values (clamped to 0-1)
            "Bridge/Key Gate Avg": avg_key_gate,
            "Bridge/Value Gate Avg": avg_value_gate,
            "Bridge/Key Gates On": key_on,
            "Bridge/Value Gates On": value_on,
            # Raw gate values (before clamping)
            "Bridge/Key Raw Gate Avg": avg_key_raw,
            "Bridge/Value Raw Gate Avg": avg_value_raw,
            # Attention weight norms (is attention learning?)
            "Bridge/Key Attn Norm": avg_key_attn_norm,
            "Bridge/Value Attn Norm": avg_value_attn_norm,
            "Bridge/Key Attn Norm Min": min(key_attn_norms),
            "Bridge/Key Attn Norm Max": max(key_attn_norms),
            "Bridge/Value Attn Norm Min": min(value_attn_norms),
            "Bridge/Value Attn Norm Max": max(value_attn_norms),
            "Bridge/Key Attn Norm Std": std_key_attn_norm,
            "Bridge/Value Attn Norm Std": std_value_attn_norm,
            # Q/K/V norms
            "Bridge/Key Q Norm": avg_key_q_norm,
            "Bridge/Key K Norm": avg_key_k_norm,
            "Bridge/Key V Norm": avg_key_v_norm,
            "Bridge/Value Q Norm": avg_value_q_norm,
            "Bridge/Value K Norm": avg_value_k_norm,
            "Bridge/Value V Norm": avg_value_v_norm,
        })

        # Optionally print per-layer gates if verbose
        if print_output and self.config.get("verbose", False):
            print(f"  Key gates:   {[f'{g:.2f}' for g in key_gates]}")
            print(f"  Value gates: {[f'{g:.2f}' for g in value_gates]}")

    def _perform_eval(self, pbar_ref=None):
        if pbar_ref: pbar_ref.clear()
        print(f"\n--- Evaluation at Step {self.global_step} ---")

        # 1. Standard Metrics
        val_loss = self.evaluator.evaluate_loss(self.val_loader)
        # Unpack 3 values now
        mmlu_acc, mmlu_err, mmlu_lat = self.mmlu_evaluator.evaluate_accuracy(self.mmlu_loader)

        print(f"Validation/Loss: {val_loss:.4f}")
        print(f"MMLU Accuracy: {mmlu_acc:.2%}")
        print(f"MMLU Latency:  {mmlu_lat:.1f}ms")

        # Log bridge gate values
        self._log_bridge_stats()

        # 2. Calculate Deltas and Organize Logs
        logs = {
            # --- Folder: Validation ---
            "Validation/Loss": val_loss,

            # --- Folder: MMLU (Absolute Performance) ---
            "MMLU/Accuracy": mmlu_acc,
            "MMLU/Error Rate": mmlu_err,
            "MMLU/Latency (s)": mmlu_lat,

        }

        if "BASELINES" in self.config:
            for name, stats in self.config["BASELINES"].items():
                # Make the name readable for the chart title (e.g., "receiver_only" -> "Receiver Only")
                clean_name = name.replace("_", " ").title()

                # Accuracy Delta
                acc_delta = mmlu_acc - stats["acc"]
                # Group: Deltas -> Subgroup: Accuracy
                logs[f"Deltas/Accuracy Delta vs {clean_name}"] = acc_delta

                # Latency Delta (Bridge - Baseline)
                lat_delta = mmlu_lat - stats["latency_ms"]
                # Group: Deltas -> Subgroup: Latency
                logs[f"Deltas/Latency Delta (s) vs {clean_name}"] = lat_delta

                print(f"vs {name}: Acc {acc_delta:+.2%} | Lat {lat_delta:+.1f}ms")

        # 3. Log & Save
        wandb.log(logs)
        self.evaluator.generate_demo("Explain quantum entanglement like I'm five.", max_new_tokens=50)
        self._save_checkpoint(checkpoint_type="step", accuracy=mmlu_acc)

    def _save_checkpoint(self, checkpoint_type="step", accuracy=None):
        """
        Save bridge checkpoint with model names in filename.

        Args:
            checkpoint_type: "step" (periodic), "best" (new best accuracy), or "final" (end of training)
            accuracy: Current accuracy (used to determine if this is the best)
        """
        checkpoint_path = "drive/MyDrive/nlp/checkpoints"
        os.makedirs(checkpoint_path, exist_ok=True)

        # Always save the step checkpoint
        if checkpoint_type == "step":
            ckpt_path = f"{checkpoint_path}/{self.checkpoint_prefix}_step_{self.global_step}.pt"
            torch.save({
                'step': self.global_step,
                'bridge_state_dict': self.bridge.state_dict(),
                'accuracy': accuracy,
                'config': self.config
            }, ckpt_path)
            print(f"Saved checkpoint: {ckpt_path}")

            # Check if this is the best accuracy
            if accuracy is not None and accuracy > self.best_accuracy:
                self.best_accuracy = accuracy
                best_path = f"{checkpoint_path}/{self.checkpoint_prefix}_best.pt"
                torch.save({
                    'step': self.global_step,
                    'bridge_state_dict': self.bridge.state_dict(),
                    'accuracy': accuracy,
                    'config': self.config
                }, best_path)
                print(f"New best accuracy ({accuracy:.2%})! Saved: {best_path}")

        elif checkpoint_type == "final":
            ckpt_path = f"{checkpoint_path}/{self.checkpoint_prefix}_final.pt"
            torch.save({
                'step': self.global_step,
                'bridge_state_dict': self.bridge.state_dict(),
                'best_accuracy': self.best_accuracy,
                'config': self.config
            }, ckpt_path)
            print(f"Saved final checkpoint: {ckpt_path}")

        print("-" * 30)

    def load_checkpoint(self, checkpoint_path):
        """
        Load bridge weights from a checkpoint file.

        Args:
            checkpoint_path: Path to the checkpoint file (.pt)

        Returns:
            dict: Checkpoint metadata (step, accuracy, config)
        """
        if not os.path.exists(checkpoint_path):
            raise FileNotFoundError(f"Checkpoint not found: {checkpoint_path}")

        print(f"Loading checkpoint: {checkpoint_path}")
        checkpoint = torch.load(checkpoint_path, map_location=self.factory.device)

        # Handle both old format (just state_dict) and new format (dict with metadata)
        if isinstance(checkpoint, dict) and 'bridge_state_dict' in checkpoint:
            self.bridge.load_state_dict(checkpoint['bridge_state_dict'])
            self.global_step = checkpoint.get('step', 0)
            loaded_acc = checkpoint.get('accuracy') or checkpoint.get('best_accuracy', 0)
            self.best_accuracy = loaded_acc if loaded_acc else 0.0
            print(f"Loaded from step {self.global_step}, accuracy: {self.best_accuracy:.2%}")
            return checkpoint
        else:
            # Old format: just the state dict
            self.bridge.load_state_dict(checkpoint)
            print("Loaded checkpoint (legacy format, no metadata)")
            return {'bridge_state_dict': checkpoint}

"""#Experiments"""

def calculate_baselines_once(config):
    print(">>> INITIALIZING FOR BASELINES...")
    # 1. Setup minimal components
    factory = H2CModelFactory(config["SHARER_ID"], config["RECEIVER_ID"])
    tok_sharer, tok_receiver = factory.load_tokenizers()
    sharer, receiver = factory.load_llms()
    bridge = factory.create_bridge() # Needed for class init, though unused in baselines

    # 2. Data
    dm = H2CDataModule(tok_sharer, tok_receiver, config)
    mmlu_loader = dm.get_mmlu_loader()

    # 3. Evaluator
    evaluator = H2CMMLUEvaluator(
        sharer, receiver, bridge, tok_receiver, tok_sharer, config, device=factory.device
    )

    # 4. Run
    baseline_results = evaluator.evaluate_baselines(mmlu_loader)

    print("\n" + "#"*50)
    print("COPY THIS INTO YOUR CONFIG:")
    print(json.dumps(baseline_results, indent=4))
    print("#"*50)

    clear_gpu()
    return baseline_results

"""## Config"""

# --- Silence HuggingFace Noise ---
datasets.logging.set_verbosity_error()
datasets.disable_progress_bar()

config = {
    "SHARER_ID" : "meta-llama/Llama-3.1-8B-Instruct",
    "RECEIVER_ID" : "Qwen/Qwen2.5-0.5B-Instruct",

    # number of steps is max_samples // batch_size
    "MAX_SAMPLES" : 100_000, # max samples of OpenHermes to pretrain bridge on
    "BATCH_SIZE"  : 8,
    "lr"          : 1e-4,

    # one step is one forward/backward pass
    # set to 10_000, its at 100 for debug
    "eval_every"  : 1000,   # eval every x steps (more frequent to catch issues)
    "log_bridge_every": 50, # log bridge gate stats every x steps (to wandb only)


    "gate_warmup_steps": 0,
    "mmlu_sample_size": 5, # number of samples from each category in MMLU to take
    "verbose"     : True, # True prints debug output in eval

    "BASELINES": {
          "receiver_only": {
              "acc": 0.3590994371482176,
              "err": 0.013696060037523453,
              "latency_ms": 0.022912156872633028
          },
          "sharer_only": {
              "acc": 0.6872420262664165,
              "err": 0.00075046904315197,
              "latency_ms": 0.08787205532388884
          },
          "text_to_text": {
              "acc": 0.38574108818011255,
              "err": 0.0031894934333958724,
              "latency_ms": 0.8592565691269808
          }
      }
}
# calculate_baselines_once(config)



"""```
[receiver_only] Acc: 35.91% | Err: 1.37% | Latency: 0.0ms
--- Running Baseline: sharer_only ---
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
[sharer_only] Acc: 68.72% | Err: 0.08% | Latency: 0.1ms
--- Running Baseline: text_to_text ---
[text_to_text] Acc: 38.57% | Err: 0.32% | Latency: 0.9ms

##################################################
COPY THIS INTO YOUR CONFIG:
{
    "receiver_only": {
        "acc": 0.3590994371482176,
        "err": 0.013696060037523453,
        "latency_ms": 0.022912156872633028
    },
    "sharer_only": {
        "acc": 0.6872420262664165,
        "err": 0.00075046904315197,
        "latency_ms": 0.08787205532388884
    },
    "text_to_text": {
        "acc": 0.38574108818011255,
        "err": 0.0031894934333958724,
        "latency_ms": 0.8592565691269808
    }
}
##################################################
Cleared GPU cache.
{'receiver_only': {'acc': 0.3590994371482176,
  'err': 0.013696060037523453,
  'latency_ms': 0.022912156872633028},
 'sharer_only': {'acc': 0.6872420262664165,
  'err': 0.00075046904315197,
  'latency_ms': 0.08787205532388884},
 'text_to_text': {'acc': 0.38574108818011255,
  'err': 0.0031894934333958724,
  'latency_ms': 0.8592565691269808}}


```


"""

clear_gpu()
# 2. Init Factory (Loads Tokenizers immediately, models later)
print(">>> 1. Initializing Factory...")
factory = H2CModelFactory(config["SHARER_ID"], config["RECEIVER_ID"])
tok_sharer, tok_receiver = factory.load_tokenizers()

# 3. Init Data Module (Prepares Splits)
print(">>> 2. Initializing Data...")
dm = H2CDataModule(tok_sharer, tok_receiver, config)

# 4. Init Engine (Loads Models & Wires everything)
# To resume from checkpoint, add: checkpoint_path="drive/MyDrive/nlp/checkpoints/bridge_..._best.pt"
print(">>> 3. Initializing Engine...")
ckpt = "drive/MyDrive/nlp/checkpoints/bridge_Llama-3-1-8B-Instruct_TO_Qwen2-5-0-5B-Instruct_step_11000.pt"
wandb_run_id = "hf7hk9ss"
# ckpt = None
# wandb_run_id = None
engine = H2CEngine(factory, dm, config, lr=config["lr"], eval_every=config["eval_every"], checkpoint_path=ckpt, wandb_run_id=wandb_run_id)#, checkpoint_path="drive/MyDrive/nlp/checkpoints/bridge_Llama-3-1-8B-Instruct_TO_Qwen2-5-0-5B-Instruct_best.pt")
wandb.watch(engine.bridge, log_freq=50)

# # 5. Run
# print(">>> 4. RUNNING...")

# engine.run(epochs=1)

"""# Visualization"""

"""# Visualization"""

# Plotly for Sankey diagrams (optional - graceful fallback if not installed)
try:
    import plotly.graph_objects as go
    PLOTLY_AVAILABLE = True
except ImportError:
    PLOTLY_AVAILABLE = False
    print("[Viz] Warning: plotly not installed. Sankey diagrams will be skipped.")


# =============================================================================
# THEME CONFIGURATION
# =============================================================================
# Dual-theme system: Dark for presentations, Light for publications.

# Helper descriptor for class properties (must be defined before Theme class)
class classproperty:
    """Descriptor that allows properties on classes (not just instances)."""
    def __init__(self, func):
        self.func = func
    def __get__(self, obj, owner):
        return self.func(owner)


class ThemeDark:
    """Dark theme - ideal for presentations and WandB dashboards."""
    NAME = "dark"

    # Core palette - deep navy background with vibrant accents
    BG_DARK = "#0D1117"
    BG_CARD = "#161B22"
    BG_ELEVATED = "#21262D"

    # Accent colors - carefully chosen for contrast and harmony
    ACCENT_PRIMARY = "#58A6FF"    # Electric blue - our method
    ACCENT_SUCCESS = "#3FB950"    # Emerald green - correct/positive
    ACCENT_WARNING = "#D29922"    # Amber - caution
    ACCENT_DANGER = "#F85149"     # Coral red - error/baseline
    ACCENT_PURPLE = "#A371F7"     # Lavender - secondary accent
    ACCENT_CYAN = "#39D5FF"       # Bright cyan - highlights

    # Text colors
    TEXT_PRIMARY = "#E6EDF3"
    TEXT_SECONDARY = "#8B949E"
    TEXT_MUTED = "#484F58"

    # Grid and borders
    GRID = "#30363D"
    BORDER = "#30363D"

    # Method-specific colors
    COLORS = {
        "H2C Bridge (Ours)": "#58A6FF",
        "Receiver Only": "#8B949E",
        "Sharer Only": "#A371F7",
        "Text To Text": "#D29922",
    }

    # Gradient definitions for heatmaps
    GRADIENT_COOL = ["#0D1117", "#1A3A5C", "#2E6B8A", "#58A6FF"]
    GRADIENT_WARM = ["#0D1117", "#4A2545", "#8B3A62", "#F85149"]
    GRADIENT_VIRIDIS_DARK = ["#0D1117", "#1E4D5C", "#2A9D8F", "#39D5FF"]

    DPI = 300
    FONT_FAMILY = "sans-serif"


class ThemeLight:
    """Light theme - ideal for academic publications and papers."""
    NAME = "light"

    # Core palette - clean white/light gray backgrounds
    BG_DARK = "#FFFFFF"
    BG_CARD = "#FAFBFC"
    BG_ELEVATED = "#F6F8FA"

    # Accent colors - slightly deeper for print readability
    ACCENT_PRIMARY = "#0969DA"    # Strong blue - our method
    ACCENT_SUCCESS = "#1A7F37"    # Forest green - correct/positive
    ACCENT_WARNING = "#9A6700"    # Dark amber - caution
    ACCENT_DANGER = "#CF222E"     # Deep red - error/baseline
    ACCENT_PURPLE = "#8250DF"     # Rich purple - secondary accent
    ACCENT_CYAN = "#0550AE"       # Deep cyan - highlights

    # Text colors
    TEXT_PRIMARY = "#1F2328"
    TEXT_SECONDARY = "#656D76"
    TEXT_MUTED = "#8C959F"

    # Grid and borders
    GRID = "#D0D7DE"
    BORDER = "#D0D7DE"

    # Method-specific colors (print-friendly)
    COLORS = {
        "H2C Bridge (Ours)": "#0969DA",
        "Receiver Only": "#656D76",
        "Sharer Only": "#8250DF",
        "Text To Text": "#9A6700",
    }

    # Gradient definitions for heatmaps (light backgrounds)
    GRADIENT_COOL = ["#FFFFFF", "#C8E1FF", "#54AEFF", "#0969DA"]
    GRADIENT_WARM = ["#FFFFFF", "#FFCDD2", "#EF9A9A", "#CF222E"]
    GRADIENT_VIRIDIS_DARK = ["#FFFFFF", "#B2DFDB", "#4DB6AC", "#00796B"]

    DPI = 300
    FONT_FAMILY = "sans-serif"


class Theme:
    """
    Dynamic theme proxy that delegates to the currently active theme.
    Use set_theme('dark') or set_theme('light') to switch.
    """
    _current = ThemeDark  # Default to dark theme

    @classmethod
    def set_theme(cls, theme_name):
        """Switch between 'dark' and 'light' themes."""
        if theme_name == "dark":
            cls._current = ThemeDark
        elif theme_name == "light":
            cls._current = ThemeLight
        else:
            raise ValueError(f"Unknown theme: {theme_name}. Use 'dark' or 'light'.")
        _apply_theme()
        print(f"[Viz] Theme switched to: {theme_name}")

    @classmethod
    def get_theme_name(cls):
        return cls._current.NAME

    # Delegate all attributes to current theme
    def __class_getitem__(cls, key):
        return getattr(cls._current, key)

    # Properties that delegate to current theme
    @classproperty
    def NAME(cls): return cls._current.NAME
    @classproperty
    def BG_DARK(cls): return cls._current.BG_DARK
    @classproperty
    def BG_CARD(cls): return cls._current.BG_CARD
    @classproperty
    def BG_ELEVATED(cls): return cls._current.BG_ELEVATED
    @classproperty
    def ACCENT_PRIMARY(cls): return cls._current.ACCENT_PRIMARY
    @classproperty
    def ACCENT_SUCCESS(cls): return cls._current.ACCENT_SUCCESS
    @classproperty
    def ACCENT_WARNING(cls): return cls._current.ACCENT_WARNING
    @classproperty
    def ACCENT_DANGER(cls): return cls._current.ACCENT_DANGER
    @classproperty
    def ACCENT_PURPLE(cls): return cls._current.ACCENT_PURPLE
    @classproperty
    def ACCENT_CYAN(cls): return cls._current.ACCENT_CYAN
    @classproperty
    def TEXT_PRIMARY(cls): return cls._current.TEXT_PRIMARY
    @classproperty
    def TEXT_SECONDARY(cls): return cls._current.TEXT_SECONDARY
    @classproperty
    def TEXT_MUTED(cls): return cls._current.TEXT_MUTED
    @classproperty
    def GRID(cls): return cls._current.GRID
    @classproperty
    def BORDER(cls): return cls._current.BORDER
    @classproperty
    def COLORS(cls): return cls._current.COLORS
    @classproperty
    def GRADIENT_COOL(cls): return cls._current.GRADIENT_COOL
    @classproperty
    def GRADIENT_WARM(cls): return cls._current.GRADIENT_WARM
    @classproperty
    def GRADIENT_VIRIDIS_DARK(cls): return cls._current.GRADIENT_VIRIDIS_DARK
    @classproperty
    def DPI(cls): return cls._current.DPI
    @classproperty
    def FONT_FAMILY(cls): return cls._current.FONT_FAMILY


def _apply_theme():
    """Apply the current theme to matplotlib globally."""
    plt.rcParams.update({
        # Figure
        'figure.facecolor': Theme.BG_DARK,
        'figure.edgecolor': Theme.BG_DARK,
        'figure.dpi': Theme.DPI,

        # Axes
        'axes.facecolor': Theme.BG_CARD,
        'axes.edgecolor': Theme.BORDER,
        'axes.labelcolor': Theme.TEXT_PRIMARY,
        'axes.titlecolor': Theme.TEXT_PRIMARY,
        'axes.linewidth': 1.2,
        'axes.spines.top': False,
        'axes.spines.right': False,
        'axes.titleweight': 'bold',
        'axes.titlesize': 14,
        'axes.labelsize': 11,
        'axes.labelweight': 'medium',

        # Grid
        'axes.grid': True,
        'grid.color': Theme.GRID,
        'grid.alpha': 0.5,
        'grid.linewidth': 0.5,
        'grid.linestyle': '-',

        # Ticks
        'xtick.color': Theme.TEXT_SECONDARY,
        'ytick.color': Theme.TEXT_SECONDARY,
        'xtick.labelsize': 10,
        'ytick.labelsize': 10,

        # Text
        'text.color': Theme.TEXT_PRIMARY,
        'font.family': Theme.FONT_FAMILY,

        # Legend
        'legend.facecolor': Theme.BG_ELEVATED,
        'legend.edgecolor': Theme.BORDER,
        'legend.labelcolor': Theme.TEXT_PRIMARY,
        'legend.fontsize': 10,
        'legend.framealpha': 0.9,

        # Save
        'savefig.facecolor': Theme.BG_DARK,
        'savefig.edgecolor': Theme.BG_DARK,
    })

    # Seaborn context
    sns.set_context("notebook", font_scale=1.1)

# Initialize with dark theme
_apply_theme()


def _create_custom_cmap(colors, name="custom"):
    """Create a custom colormap from a list of hex colors."""
    return LinearSegmentedColormap.from_list(name, colors, N=256)


def _add_glow(artist, color, alpha=0.3, linewidth=3):
    """Add a subtle glow/shadow effect to a matplotlib artist (text, line, etc.)."""
    artist.set_path_effects([
        path_effects.SimpleLineShadow(offset=(0, 0), shadow_color=color, alpha=alpha, linewidth=linewidth),
        path_effects.Normal()
    ])


def _add_text_stroke(artist, stroke_color=None, linewidth=3):
    """Add a stroke/outline effect to text for better readability on busy backgrounds."""
    if stroke_color is None:
        stroke_color = Theme.BG_DARK
    artist.set_path_effects([
        path_effects.withStroke(linewidth=linewidth, foreground=stroke_color),
    ])


def _format_card(ax, title=None, subtitle=None):
    """Apply card-like styling to an axes with optional title/subtitle."""
    ax.set_facecolor(Theme.BG_CARD)

    # Add subtle border effect by setting spine properties
    for spine in ax.spines.values():
        spine.set_color(Theme.BORDER)
        spine.set_linewidth(1)

    if title:
        # Increased pad from 15 to 22 to make room for subtitle
        ax.set_title(title, fontsize=14, fontweight='bold',
                     color=Theme.TEXT_PRIMARY, pad=22, loc='left')
    if subtitle:
        # Moved up from 1.02 to 1.01 (sits just below the title)
        ax.text(0.0, 1.01, subtitle, transform=ax.transAxes,
                fontsize=9, color=Theme.TEXT_SECONDARY, style='italic')


def _add_caption(fig, text, fontsize=9):
    """Add a caption below the figure for paper-readiness."""
    fig.text(0.5, 0.01, text, ha='center', fontsize=fontsize,
             color=Theme.TEXT_SECONDARY, style='italic', wrap=True)


# Subject category mappings for MMLU analysis
SUBJECT_CATEGORIES = {
    "STEM": [
        'abstract_algebra', 'astronomy', 'college_biology', 'college_chemistry',
        'college_computer_science', 'college_mathematics', 'college_physics',
        'computer_security', 'conceptual_physics', 'electrical_engineering',
        'elementary_mathematics', 'high_school_biology', 'high_school_chemistry',
        'high_school_computer_science', 'high_school_mathematics', 'high_school_physics',
        'high_school_statistics', 'machine_learning', 'anatomy', 'college_medicine',
        'clinical_knowledge', 'medical_genetics', 'professional_medicine', 'virology', 'nutrition'
    ],
    "Humanities": [
        'formal_logic', 'high_school_european_history', 'high_school_us_history',
        'high_school_world_history', 'international_law', 'jurisprudence', 'logical_fallacies',
        'moral_disputes', 'moral_scenarios', 'philosophy', 'prehistory', 'professional_law',
        'world_religions'
    ],
    "Social Sciences": [
        'econometrics', 'high_school_geography', 'high_school_government_and_politics',
        'high_school_macroeconomics', 'high_school_microeconomics', 'high_school_psychology',
        'human_sexuality', 'professional_psychology', 'public_relations', 'security_studies',
        'sociology', 'us_foreign_policy', 'human_aging'
    ],
    "Other": [
        'business_ethics', 'global_facts', 'management', 'marketing', 'miscellaneous',
        'professional_accounting'
    ]
}


def _categorize_subject(subject):
    """Map an MMLU subject to its broad category."""
    for category, subjects in SUBJECT_CATEGORIES.items():
        if subject in subjects:
            return category
    return "Other"


def _safe_viz(func):
    """Decorator for graceful error handling in visualization functions."""
    def wrapper(*args, **kwargs):
        try:
            return func(*args, **kwargs)
        except Exception as e:
            print(f"[Viz] Warning: {func.__name__} failed with error: {e}")
            return None
    return wrapper


# =============================================================================
# VISUALIZATION FUNCTIONS
# =============================================================================

@_safe_viz
def log_performance_charts(engine, config, eval_cache=None, baseline_results=None):
    """
    Generate performance comparison visualizations.
    Creates: (1) Scatter plot of accuracy vs latency, (2) Horizontal bar chart.

    Args:
        eval_cache: Optional dict with keys 'acc', 'err', 'lat' to avoid re-evaluation
        baseline_results: Optional dict from evaluate_baselines_detailed() - preferred source
    """
    print("[Viz] Generating performance charts...")
    _apply_theme()

    # --- Data Preparation ---
    data = []

    # Prefer freshly calculated baseline_results over config["BASELINES"]
    if baseline_results:
        for name, metrics in baseline_results.items():
            clean_name = name.replace("_", " ").title()
            lat = metrics.get("latency_ms", 0)
            if lat > 10:  # Assume ms if > 10
                lat = lat / 1000.0
            data.append({
                "Method": clean_name,
                "Accuracy": metrics["acc"],
                "Latency": lat,
                "Type": "baseline"
            })
    else:
        # Fallback to config baselines
        for name, metrics in config.get("BASELINES", {}).items():
            clean_name = name.replace("_", " ").title()
            lat = metrics.get("latency_sec", metrics.get("latency_ms", 0))
            if lat > 10:  # Assume ms if > 10
                lat = lat / 1000.0
            data.append({
                "Method": clean_name,
                "Accuracy": metrics["acc"],
                "Latency": lat,
                "Type": "baseline"
            })

    # Add our method - use cache if available
    if eval_cache:
        h2c_acc, h2c_lat = eval_cache['acc'], eval_cache['lat']
    else:
        h2c_acc, _, h2c_lat = engine.mmlu_evaluator.evaluate_accuracy(engine.mmlu_loader)

    data.append({
        "Method": "H2C Bridge (Ours)",
        "Accuracy": h2c_acc,
        "Latency": h2c_lat,
        "Type": "ours"
    })
    df = pd.DataFrame(data)

    # ==========================================================================
    # CHART 1: Accuracy vs Latency Scatter
    # ==========================================================================
    fig1, ax = plt.subplots(figsize=(10, 6))
    _format_card(ax, title="Performance Trade-off",
                 subtitle="Accuracy vs. Inference Latency")

    # Plot baselines first (smaller, muted)
    baselines = df[df["Type"] == "baseline"]
    ax.scatter(
        baselines["Latency"], baselines["Accuracy"],
        s=120, c=Theme.TEXT_MUTED, alpha=0.7,
        edgecolors=Theme.TEXT_SECONDARY, linewidth=1.5,
        zorder=2, label="Baselines"
    )

    # Plot our method (larger, glowing)
    ours = df[df["Type"] == "ours"]
    scatter_ours = ax.scatter(
        ours["Latency"], ours["Accuracy"],
        s=300, c=Theme.ACCENT_PRIMARY, alpha=1.0,
        edgecolors='white', linewidth=2,
        zorder=3, marker='D', label="H2C Bridge (Ours)"
    )

    # Add glow effect to our point
    ax.scatter(
        ours["Latency"], ours["Accuracy"],
        s=600, c=Theme.ACCENT_PRIMARY, alpha=0.15,
        zorder=1
    )

    # Annotations
    for _, row in df.iterrows():
        is_ours = row["Type"] == "ours"
        offset = (-12, 18) if is_ours else (8, -12)
        color = Theme.ACCENT_PRIMARY if is_ours else Theme.TEXT_SECONDARY
        weight = 'bold' if is_ours else 'normal'

        ax.annotate(
            row["Method"],
            (row["Latency"], row["Accuracy"]),
            xytext=offset, textcoords='offset points',
            fontsize=9, fontweight=weight, color=color,
            ha='center' if is_ours else 'left',
            path_effects=[path_effects.withStroke(linewidth=3, foreground=Theme.BG_CARD)]
        )

    # Ideal region indicator (top-left corner)
    ax.annotate(
        "IDEAL",
        xy=(0.02, 0.98), xycoords='axes fraction',
        fontsize=8, color=Theme.ACCENT_SUCCESS, alpha=0.6,
        ha='left', va='top', fontweight='bold'
    )
    ax.annotate(
        "",
        xy=(0.02, 0.95), xycoords='axes fraction',
        xytext=(0.15, 0.85), textcoords='axes fraction',
        arrowprops=dict(arrowstyle="->", color=Theme.ACCENT_SUCCESS, alpha=0.3, lw=1.5)
    )

    ax.set_xlabel("Latency (seconds) - lower is better", labelpad=10)
    ax.set_ylabel("Accuracy - higher is better", labelpad=10)
    ax.legend(loc='lower right', framealpha=0.9)

    plt.tight_layout()
    wandb.log({"Evaluation/Performance_Scatter": wandb.Image(fig1)})
    plt.close(fig1)

    # ==========================================================================
    # CHART 2: Horizontal Bar Chart
    # ==========================================================================
    fig2, ax = plt.subplots(figsize=(9, 5))
    _format_card(ax, title="Accuracy Comparison")

    # Sort by accuracy
    df_sorted = df.sort_values("Accuracy", ascending=True)

    # Create bars with conditional coloring
    colors = [Theme.ACCENT_PRIMARY if t == "ours" else Theme.TEXT_MUTED
              for t in df_sorted["Type"]]

    bars = ax.barh(
        df_sorted["Method"], df_sorted["Accuracy"],
        color=colors, edgecolor=Theme.BG_ELEVATED, linewidth=1,
        height=0.6, alpha=0.9
    )

    # Add value labels
    for bar, acc in zip(bars, df_sorted["Accuracy"]):
        width = bar.get_width()
        is_ours = width == ours["Accuracy"].values[0]

        # Label inside or outside based on bar width
        if width > 0.15:
            ax.text(
                width - 0.02, bar.get_y() + bar.get_height()/2,
                f"{acc:.1%}",
                ha='right', va='center',
                fontsize=11, fontweight='bold',
                color='white' if is_ours else Theme.BG_CARD
            )
        else:
            ax.text(
                width + 0.01, bar.get_y() + bar.get_height()/2,
                f"{acc:.1%}",
                ha='left', va='center',
                fontsize=11, fontweight='bold',
                color=Theme.TEXT_PRIMARY
            )

    # Add highlight for our method
    for bar, method_type in zip(bars, df_sorted["Type"]):
        if method_type == "ours":
            # Glow effect
            ax.barh(
                bar.get_y() + bar.get_height()/2,
                bar.get_width(),
                height=bar.get_height() * 1.3,
                color=Theme.ACCENT_PRIMARY, alpha=0.15,
                zorder=0
            )

    ax.set_xlabel("Accuracy", labelpad=10)
    ax.set_xlim(0, max(df["Accuracy"]) * 1.15)
    ax.xaxis.grid(True, alpha=0.3)
    ax.yaxis.grid(False)
    ax.spines['left'].set_visible(False)
    ax.tick_params(axis='y', length=0)

    plt.tight_layout()
    wandb.log({"Evaluation/Accuracy_Bars": wandb.Image(fig2)})
    plt.close(fig2)

    print("[Viz] Performance charts logged.")


@_safe_viz
def log_gate_dynamics(engine):
    """
    Visualize learned gating mechanisms across layers.
    Shows how the bridge modulates information flow.
    """
    print("[Viz] Generating gate dynamics...")
    _apply_theme()

    bridge = engine.bridge
    k_gates = [torch.sigmoid(layer.gate).detach().cpu().item()
               for layer in bridge.key_modifiers]
    v_gates = [torch.sigmoid(layer.gate).detach().cpu().item()
               for layer in bridge.value_modifiers]
    layers = np.arange(len(k_gates))

    fig, ax = plt.subplots(figsize=(11, 5))
    _format_card(ax, title="Learned Gate Strengths by Layer",
                 subtitle="0 = no contribution, 1 = full contribution from bridge")

    # Reference lines at 0, 0.5, and 1
    ax.axhline(0.5, color=Theme.TEXT_MUTED, linewidth=1, linestyle='--', alpha=0.5, zorder=1)

    # Key gates - with fill
    line_k, = ax.plot(layers, k_gates,
                      marker='o', markersize=8, markeredgecolor='white', markeredgewidth=1.5,
                      linestyle='-', linewidth=2.5,
                      color=Theme.ACCENT_PRIMARY, label='Key Modifier', zorder=3)
    ax.fill_between(layers, k_gates, 0,
                    color=Theme.ACCENT_PRIMARY, alpha=0.15, zorder=2)

    # Value gates - with fill
    line_v, = ax.plot(layers, v_gates,
                      marker='s', markersize=7, markeredgecolor='white', markeredgewidth=1.5,
                      linestyle='--', linewidth=2.5,
                      color=Theme.ACCENT_WARNING, label='Value Modifier', zorder=3)
    ax.fill_between(layers, v_gates, 0,
                    color=Theme.ACCENT_WARNING, alpha=0.1, zorder=2)

    # Styling
    ax.set_xlabel("Layer Index", labelpad=10)
    ax.set_ylabel("Gate Strength", labelpad=10)
    ax.set_ylim(-0.05, 1.1)
    ax.set_xticks(layers)

    # Add region labels
    ax.text(0.98, 0.95, "HIGH CONTRIBUTION", transform=ax.transAxes,
            fontsize=8, color=Theme.ACCENT_SUCCESS, alpha=0.5,
            ha='right', va='top', fontweight='bold')
    ax.text(0.98, 0.05, "LOW CONTRIBUTION", transform=ax.transAxes,
            fontsize=8, color=Theme.ACCENT_WARNING, alpha=0.5,
            ha='right', va='bottom', fontweight='bold')

    # Statistics box (enhancement)
    k_arr, v_arr = np.array(k_gates), np.array(v_gates)
    stats_text = (
        f"Key:   mean={k_arr.mean():.3f}  std={k_arr.std():.3f}  range=[{k_arr.min():.2f}, {k_arr.max():.2f}]\n"
        f"Value: mean={v_arr.mean():.3f}  std={v_arr.std():.3f}  range=[{v_arr.min():.2f}, {v_arr.max():.2f}]"
    )
    props = dict(boxstyle='round,pad=0.4', facecolor=Theme.BG_ELEVATED,
                 edgecolor=Theme.BORDER, alpha=0.9)
    ax.text(0.02, 0.98, stats_text, transform=ax.transAxes,
            fontsize=8, fontfamily='monospace', color=Theme.TEXT_SECONDARY,
            va='top', ha='left', bbox=props)

    ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.12), ncol=2)
    ax.yaxis.grid(True, alpha=0.3)
    ax.xaxis.grid(False)

    plt.tight_layout()
    wandb.log({"Interpretability/Gate_Dynamics": wandb.Image(fig)})
    plt.close(fig)

    print("[Viz] Gate dynamics logged.")


@_safe_viz
def log_bridge_heatmap(engine, prompt_text="Explain quantum entanglement."):
    """
    Visualize injection magnitudes across layers as a heatmap.
    Shows which layers are most active in knowledge transfer.
    """
    print(f"[Viz] Generating injection heatmap for: '{prompt_text[:40]}...'")
    _apply_theme()

    model = engine.bridge
    model.eval()

    k_norms, v_norms = [], []

    def make_hook(storage):
        def hook(module, inp, out):
            injection = out - inp[0]
            norm = injection.norm(p=2, dim=-1).mean().item()
            storage.append(norm)
        return hook

    handles = []
    for layer in model.key_modifiers:
        handles.append(layer.register_forward_hook(make_hook(k_norms)))
    for layer in model.value_modifiers:
        handles.append(layer.register_forward_hook(make_hook(v_norms)))

    with torch.no_grad():
        engine.evaluator.generate_demo(prompt_text, max_new_tokens=1)

    for h in handles:
        h.remove()

    # Build heatmap data
    data = np.array([k_norms, v_norms])

    # Larger figure for better readability
    num_layers = len(k_norms)
    fig_width = max(14, num_layers * 0.8)
    fig, ax = plt.subplots(figsize=(fig_width, 4.5))

    # Custom colormap: dark to cyan
    cmap = _create_custom_cmap(Theme.GRADIENT_VIRIDIS_DARK, "injection")

    # Heatmap - remove square constraint for larger cells
    im = ax.imshow(data, aspect='auto', cmap=cmap)

    # Find max injection layers
    max_k_layer = np.argmax(k_norms)
    max_v_layer = np.argmax(v_norms)

    # Annotations with max layer highlighting
    for i in range(data.shape[0]):
        for j in range(data.shape[1]):
            val = data[i, j]
            # Determine text color based on cell brightness
            text_color = Theme.BG_DARK if val > data.max() * 0.6 else Theme.TEXT_PRIMARY

            # Check if this is a max layer
            is_max = (i == 0 and j == max_k_layer) or (i == 1 and j == max_v_layer)
            fontweight = 'bold' if is_max else 'medium'
            fontsize = 10 if is_max else 9

            txt = ax.text(j, i, f"{val:.2f}",
                          ha='center', va='center',
                          fontsize=fontsize, fontweight=fontweight,
                          color=text_color)

            # Add star marker for max values
            if is_max:
                ax.plot(j, i - 0.35, marker='v', markersize=6,
                        color=Theme.ACCENT_CYAN, markeredgecolor='white', markeredgewidth=0.5)

    # Axes
    ax.set_xticks(np.arange(len(k_norms)))
    ax.set_xticklabels([str(i) for i in range(len(k_norms))])
    ax.set_yticks([0, 1])
    ax.set_yticklabels(["Key", "Value"])
    ax.set_xlabel("Layer Index", labelpad=10)

    # Title with prompt preview
    short_prompt = prompt_text[:50] + "..." if len(prompt_text) > 50 else prompt_text
    ax.set_title(f"Injection Magnitude (L2 Norm)\n\"{short_prompt}\"",
                 fontsize=12, fontweight='bold', pad=15, loc='left',
                 color=Theme.TEXT_PRIMARY)

    # Max layer annotation
    ax.text(0.99, 0.98, f"Peak: K@L{max_k_layer}, V@L{max_v_layer}",
            transform=ax.transAxes, fontsize=8, color=Theme.ACCENT_CYAN,
            ha='right', va='top', fontweight='bold')

    # Colorbar
    cbar = plt.colorbar(im, ax=ax, pad=0.02, aspect=15)
    cbar.set_label("L2 Norm", fontsize=10, color=Theme.TEXT_SECONDARY)
    cbar.ax.yaxis.set_tick_params(color=Theme.TEXT_SECONDARY)
    plt.setp(plt.getp(cbar.ax.axes, 'yticklabels'), color=Theme.TEXT_SECONDARY)

    # Remove spines for cleaner look
    for spine in ax.spines.values():
        spine.set_visible(False)

    plt.tight_layout()
    wandb.log({"Interpretability/Injection_Heatmap": wandb.Image(fig)})
    plt.close(fig)

    print("[Viz] Injection heatmap logged.")


@_safe_viz
def log_probability_shift(engine, prompt_text, true_label="A"):
    """
    Visualize how the bridge changes answer probabilities.
    Compares baseline receiver vs. bridge-augmented predictions.
    """
    print(f"[Viz] Generating probability shift chart...")
    _apply_theme()

    # --- Compute probabilities ---
    full_prompt = f"{prompt_text}\nSelect the correct answer. Output ONLY the answer letter (A, B, C, or D). Answer:"
    tok = engine.factory.tok_receiver
    sharer_tok = engine.factory.tok_sharer
    candidates = ["A", "B", "C", "D"]
    candidate_ids = [tok.encode(" " + c, add_special_tokens=False)[-1] for c in candidates]

    s_ids = sharer_tok.apply_chat_template(
        [{"role": "user", "content": full_prompt}],
        return_tensors="pt", add_generation_prompt=True
    ).to(engine.factory.device)[:, :-1]

    r_ids = tok.apply_chat_template(
        [{"role": "user", "content": full_prompt}],
        return_tensors="pt", add_generation_prompt=True
    ).to(engine.factory.device)
    rec_context = r_ids[:, :-1]

    def get_probs(logits):
        target_logits = logits[0, -1, candidate_ids]
        return F.softmax(target_logits, dim=0).float().cpu().numpy()

    with torch.no_grad(), torch.autocast(device_type="cuda", dtype=torch.bfloat16):
        out_base = engine.receiver(r_ids)
        probs_base = get_probs(out_base.logits)

        mod_cache = engine.evaluator.get_bridged_cache(s_ids, torch.ones_like(s_ids), rec_context)
        out_bridge = engine.receiver(r_ids, past_key_values=mod_cache)
        probs_bridge = get_probs(out_bridge.logits)

    # --- Visualization ---
    fig, ax = plt.subplots(figsize=(11, 6))
    _format_card(ax, title="Answer Probability Distribution",
                 subtitle="How the bridge shifts model confidence")

    x = np.arange(len(candidates))
    width = 0.30  # Slightly narrower bars to give more space

    # Baseline bars
    bars1 = ax.bar(x - width/2, probs_base, width,
                   label='Receiver Only',
                   color=Theme.TEXT_MUTED, alpha=0.7,
                   edgecolor=Theme.BG_ELEVATED, linewidth=1)

    # Bridge bars
    bars2 = ax.bar(x + width/2, probs_bridge, width,
                   label='With H2C Bridge',
                   color=Theme.ACCENT_PRIMARY, alpha=0.9,
                   edgecolor=Theme.BG_ELEVATED, linewidth=1)

    # Delta indicators between bars - positioned to not overlap with bar labels
    for i, (base, bridge) in enumerate(zip(probs_base, probs_bridge)):
        delta = bridge - base
        if abs(delta) > 0.03:  # Only show significant changes
            arrow_color = Theme.ACCENT_SUCCESS if delta > 0 else Theme.ACCENT_DANGER

            # Position delta label to the right of the bar pair, avoiding overlap
            label_x = i + width/2 + 0.12
            label_y = (base + bridge) / 2  # Midpoint between bars

            # Draw a small connecting line from bridge bar to delta label
            ax.plot([i + width/2 + 0.02, label_x - 0.02],
                    [bridge, label_y],
                    color=arrow_color, linewidth=1.5, alpha=0.6)

            # Delta label with arrow indicator
            delta_text = f"{delta:+.0%}"
            ax.text(label_x, label_y, delta_text,
                    ha='left', va='center', fontsize=9, fontweight='bold',
                    color=arrow_color,
                    path_effects=[path_effects.withStroke(linewidth=2, foreground=Theme.BG_CARD)])

    # Highlight correct answer
    if true_label in candidates:
        idx = candidates.index(true_label)
        # Background highlight
        ax.axvspan(idx - 0.48, idx + 0.48,
                   color=Theme.ACCENT_SUCCESS, alpha=0.08, zorder=0)
        # Border on top and bottom
        ax.axhline(y=0, xmin=(idx + 0.02)/4, xmax=(idx + 0.98)/4,
                   color=Theme.ACCENT_SUCCESS, linewidth=3, alpha=0.5)
        # Label
        ax.text(idx, 1.02, "[CORRECT]",
                ha='center', va='bottom',
                fontsize=9, fontweight='bold',
                color=Theme.ACCENT_SUCCESS)

    # Value labels on bars
    def label_bars(bars, is_bridge=False):
        for bar in bars:
            height = bar.get_height()
            if height > 0.03:
                ax.text(bar.get_x() + bar.get_width()/2, height + 0.01,
                        f'{height:.0%}',
                        ha='center', va='bottom',
                        fontsize=9, fontweight='bold',
                        color=Theme.ACCENT_PRIMARY if is_bridge else Theme.TEXT_SECONDARY)

    label_bars(bars1, False)
    label_bars(bars2, True)

    ax.set_ylabel("Probability", labelpad=10)
    ax.set_xticks(x)
    ax.set_xticklabels(candidates, fontsize=14, fontweight='bold')
    ax.set_ylim(0, 1.15)
    ax.set_xlim(-0.5, len(candidates) - 0.2)  # Room for delta labels on right
    ax.yaxis.grid(True, alpha=0.2)
    ax.xaxis.grid(False)

    ax.legend(loc='upper left')

    plt.tight_layout()
    wandb.log({"Interpretability/Probability_Shift": wandb.Image(fig)})
    plt.close(fig)

    print("[Viz] Probability shift logged.")


@_safe_viz
def log_qualitative_table(engine):
    """
    Generate a WandB table showing side-by-side generation comparisons
    across all baselines: Receiver Only, Sharer Only, Text-to-Text, and H2C Bridge.
    """
    print("[Viz] Generating qualitative comparison table...")

    # Diverse prompts to showcase different capabilities
    prompts = [
        # --- Factual Knowledge ---
        "What is the capital of Australia?",
        "Who wrote 'Pride and Prejudice'?",
        "What is the speed of light in meters per second?",
        "Name the largest planet in our solar system.",

        # --- Reasoning & Explanation ---
        "Explain 'Schrodinger's Cat' in one sentence.",
        "Why do we see lightning before we hear thunder?",
        "What causes the seasons on Earth?",
        "Explain the difference between a virus and a bacterium.",

        # --- Logic & Problem Solving ---
        "Riddle: I have no body, but I come alive with wind. What am I?",
        "If all roses are flowers and some flowers fade quickly, can we conclude that some roses fade quickly?",
        "A bat and ball cost $1.10 together. The bat costs $1.00 more than the ball. How much does the ball cost?",

        # --- Math ---
        "What is 17 * 24?",
        "What is the derivative of x^3 + 2x?",
        "If a train travels 120 miles in 2 hours, what is its average speed?",

        # --- Coding ---
        "Write a Python one-liner to reverse a string.",
        "What does the 'finally' block do in Python exception handling?",

        # --- Creative ---
        "Write a haiku about machine learning.",
        "Complete this sentence creatively: 'The robot looked at the sunset and felt...'",

        # --- Multiple Choice (MMLU-style) ---
        "What is the atomic number of carbon?\nA) 6\nB) 12\nC) 14\nD) 8\nAnswer:",

        "Which of the following is NOT a programming paradigm?\nA) Object-oriented\nB) Functional\nC) Recursive\nD) Procedural\nAnswer:",

        "The Treaty of Westphalia (1648) is most associated with:\nA) The end of World War I\nB) The establishment of the United Nations\nC) The emergence of the modern nation-state system\nD) The partition of Africa\nAnswer:",

        "In economics, 'opportunity cost' refers to:\nA) The monetary cost of a decision\nB) The value of the next best alternative foregone\nC) The cost of missed opportunities in the stock market\nD) The price of goods in a competitive market\nAnswer:",

        "Which logical fallacy involves attacking the person making an argument rather than the argument itself?\nA) Straw man\nB) Ad hominem\nC) False dichotomy\nD) Slippery slope\nAnswer:",

        "The Pythagorean theorem states that for a right triangle:\nA) a + b = c\nB) a^2 + b^2 = c^2\nC) a * b = c\nD) a^2 - b^2 = c^2\nAnswer:",
    ]

    table = wandb.Table(columns=[
        "Prompt", "Receiver Only", "Sharer Only", "Text-to-Text", "H2C Bridge"
    ])

    engine.bridge.eval()
    engine.receiver.eval()
    engine.sharer.eval()
    tok_rec = engine.factory.tok_receiver
    tok_sha = engine.factory.tok_sharer
    device = engine.factory.device

    max_tokens = 60  # Enough for most answers

    for i, prompt in enumerate(prompts):
        print(f"  [{i+1}/{len(prompts)}] Processing: {prompt[:50]}...")

        # --- Receiver Only ---
        r_input = tok_rec.apply_chat_template(
            [{"role": "user", "content": prompt}],
            return_tensors="pt", add_generation_prompt=True
        ).to(device)

        with torch.no_grad():
            out_rec = engine.receiver.generate(
                r_input, max_new_tokens=max_tokens,
                do_sample=False, pad_token_id=tok_rec.pad_token_id
            )
        text_rec = tok_rec.decode(
            out_rec[0, r_input.shape[1]:], skip_special_tokens=False
        ).strip()

        # --- Sharer Only ---
        s_input = tok_sha.apply_chat_template(
            [{"role": "user", "content": prompt}],
            return_tensors="pt", add_generation_prompt=True
        ).to(device)

        with torch.no_grad(), torch.autocast(device_type="cuda", dtype=torch.bfloat16):
            out_sha = engine.sharer.generate(
                s_input, max_new_tokens=max_tokens,
                do_sample=False, pad_token_id=tok_sha.pad_token_id
            )
        text_sha = tok_sha.decode(
            out_sha[0, s_input.shape[1]:], skip_special_tokens=False
        ).strip()

        # --- Text-to-Text (Sharer generates, Receiver continues) ---
        # Get sharer's output as text, then feed to receiver
        sharer_response = text_sha[:200]  # Limit length
        t2t_prompt = f"{prompt}\n\nContext from another model: {sharer_response}\n\nYour answer:"
        t2t_input = tok_rec.apply_chat_template(
            [{"role": "user", "content": t2t_prompt}],
            return_tensors="pt", add_generation_prompt=True
        ).to(device)

        with torch.no_grad():
            out_t2t = engine.receiver.generate(
                t2t_input, max_new_tokens=max_tokens,
                do_sample=False, pad_token_id=tok_rec.pad_token_id
            )
        text_t2t = tok_rec.decode(
            out_t2t[0, t2t_input.shape[1]:], skip_special_tokens=False
        ).strip()

        # --- H2C Bridge ---
        with torch.no_grad(), torch.autocast(device_type="cuda", dtype=torch.bfloat16):
            cache = engine.evaluator.get_bridged_cache(
                s_input[:, :-1],
                torch.ones_like(s_input[:, :-1]),
                r_input[:, :-1]
            )

            out_bridge = engine.receiver.generate(
                r_input, past_key_values=cache,
                max_new_tokens=max_tokens, do_sample=False,
                pad_token_id=tok_rec.pad_token_id
            )
        text_bridge = tok_rec.decode(
            out_bridge[0, r_input.shape[1]:], skip_special_tokens=False
        ).strip()

        # Truncate long responses for table readability
        def truncate(s, max_len=300):
            return s[:max_len] + "..." if len(s) > max_len else s

        table.add_data(
            prompt,
            truncate(text_rec),
            truncate(text_sha),
            truncate(text_t2t),
            truncate(text_bridge)
        )

    wandb.log({"Evaluation/Qualitative_Comparisons": table})
    print("[Viz] Qualitative table logged with all baselines.")


@_safe_viz
def log_training_summary(engine, config, eval_cache=None, baseline_results=None):
    """
    Generate a comprehensive summary panel combining multiple metrics.

    Args:
        eval_cache: Optional dict with keys 'acc', 'err', 'lat' to avoid re-evaluation
        baseline_results: Optional dict from evaluate_baselines_detailed() - preferred source
    """
    print("[Viz] Generating training summary...")
    _apply_theme()

    # Collect data - use cache if available
    if eval_cache:
        h2c_acc, h2c_err, h2c_lat = eval_cache['acc'], eval_cache['err'], eval_cache['lat']
    else:
        h2c_acc, h2c_err, h2c_lat = engine.mmlu_evaluator.evaluate_accuracy(engine.mmlu_loader)

    # Prefer freshly calculated baseline_results over config["BASELINES"]
    if baseline_results:
        baselines = {name: {"acc": v["acc"], "latency_ms": v["latency_ms"]}
                     for name, v in baseline_results.items()}
    else:
        baselines = config.get("BASELINES", {})

    # Create figure with subplots - increased spacing to prevent overlap
    fig = plt.figure(figsize=(16, 9))
    gs = fig.add_gridspec(2, 3, hspace=0.45, wspace=0.35)

    fig.patch.set_facecolor(Theme.BG_DARK)

    # --- Panel 1: Accuracy Gauge (top-left) ---
    ax1 = fig.add_subplot(gs[0, 0])
    ax1.set_facecolor(Theme.BG_CARD)

    # Simple radial gauge effect using a partial circle
    theta = np.linspace(0.75 * np.pi, 0.25 * np.pi, 100)
    r = 1
    x_arc = r * np.cos(theta)
    y_arc = r * np.sin(theta)

    # Background arc
    ax1.plot(x_arc, y_arc, color=Theme.TEXT_MUTED, linewidth=15, alpha=0.3)

    # Filled arc based on accuracy
    fill_idx = int(len(theta) * h2c_acc)
    ax1.plot(x_arc[:fill_idx], y_arc[:fill_idx],
             color=Theme.ACCENT_PRIMARY, linewidth=15, solid_capstyle='round')

    # Center text
    ax1.text(0, 0.15, f"{h2c_acc:.1%}", ha='center', va='center',
             fontsize=26, fontweight='bold', color=Theme.TEXT_PRIMARY)
    ax1.text(0, -0.15, "Accuracy", ha='center', va='center',
             fontsize=10, color=Theme.TEXT_SECONDARY)

    # Trend indicator vs best baseline - positioned below the gauge
    if baselines:
        best_baseline_acc = max(v["acc"] for v in baselines.values())
        delta_vs_best = h2c_acc - best_baseline_acc
        trend_color = Theme.ACCENT_SUCCESS if delta_vs_best >= 0 else Theme.ACCENT_DANGER
        trend_arrow = "+" if delta_vs_best >= 0 else ""
        ax1.text(0, -0.45, f"{trend_arrow}{delta_vs_best:.1%} vs best baseline",
                 ha='center', va='center',
                 fontsize=9, color=trend_color, fontweight='bold')

    ax1.set_xlim(-1.4, 1.4)
    ax1.set_ylim(-0.7, 1.4)
    ax1.axis('off')
    ax1.set_title("H2C Bridge Performance", fontsize=12, fontweight='bold',
                  color=Theme.TEXT_PRIMARY, pad=10)

    # --- Panel 2: Latency Comparison (top-middle) ---
    ax2 = fig.add_subplot(gs[0, 1])
    _format_card(ax2, title="Latency (seconds)")

    methods = ["H2C Bridge"] + [k.replace("_", " ").title() for k in baselines.keys()]
    latencies = [h2c_lat]
    for v in baselines.values():
        lat = v.get("latency_ms", v.get("latency_sec", 0))
        if lat > 10:
            lat = lat / 1000
        latencies.append(lat)

    colors = [Theme.ACCENT_PRIMARY] + [Theme.TEXT_MUTED] * len(baselines)

    bars = ax2.barh(methods, latencies, color=colors, height=0.5, alpha=0.8)
    ax2.set_xlabel("Seconds", labelpad=5)
    ax2.xaxis.grid(True, alpha=0.2)
    ax2.yaxis.grid(False)
    ax2.spines['left'].set_visible(False)

    # --- Panel 3: Delta vs Baselines (top-right) ---
    ax3 = fig.add_subplot(gs[0, 2])
    _format_card(ax3, title="Accuracy Delta vs Baselines")

    baseline_names = [k.replace("_", " ").title() for k in baselines.keys()]
    deltas = [h2c_acc - v["acc"] for v in baselines.values()]

    bar_colors = [Theme.ACCENT_SUCCESS if d >= 0 else Theme.ACCENT_DANGER for d in deltas]

    bars = ax3.barh(baseline_names, deltas, color=bar_colors, height=0.5, alpha=0.8)
    ax3.axvline(0, color=Theme.TEXT_MUTED, linewidth=1, linestyle='--')
    ax3.set_xlabel("Delta (percentage points)", labelpad=5)
    ax3.xaxis.grid(True, alpha=0.2)
    ax3.yaxis.grid(False)
    ax3.spines['left'].set_visible(False)

    # Value labels
    for bar, delta in zip(bars, deltas):
        xpos = bar.get_width() + 0.005 if delta >= 0 else bar.get_width() - 0.005
        ha = 'left' if delta >= 0 else 'right'
        ax3.text(xpos, bar.get_y() + bar.get_height()/2,
                 f"{delta:+.1%}", ha=ha, va='center',
                 fontsize=9, fontweight='bold',
                 color=Theme.ACCENT_SUCCESS if delta >= 0 else Theme.ACCENT_DANGER)

    # --- Panel 4: Gate Distribution (bottom, spanning 2 columns) ---
    ax4 = fig.add_subplot(gs[1, :2])
    _format_card(ax4, title="Gate Value Distribution")

    k_gates = [layer.gate.clamp(0.0, 1.0).detach().cpu().item()
               for layer in engine.bridge.key_modifiers]
    v_gates = [layer.gate.clamp(0.0, 1.0).detach().cpu().item()
               for layer in engine.bridge.value_modifiers]

    all_gates = k_gates + v_gates
    labels = ["K"] * len(k_gates) + ["V"] * len(v_gates)

    # Violin plot
    parts = ax4.violinplot([k_gates, v_gates], positions=[0, 1],
                           showmeans=True, showmedians=False)

    for i, pc in enumerate(parts['bodies']):
        pc.set_facecolor(Theme.ACCENT_PRIMARY if i == 0 else Theme.ACCENT_WARNING)
        pc.set_alpha(0.6)

    # Style the statistical lines (check keys exist for compatibility)
    for key in ['cmeans', 'cbars', 'cmins', 'cmaxs']:
        if key in parts:
            color = Theme.TEXT_PRIMARY if key == 'cmeans' else Theme.TEXT_MUTED
            parts[key].set_color(color)

    ax4.set_xticks([0, 1])
    ax4.set_xticklabels(["Key Gates", "Value Gates"])
    ax4.set_ylabel("Gate Strength (0-1)")
    ax4.axhline(0.5, color=Theme.TEXT_MUTED, linewidth=1, linestyle='--', alpha=0.5)
    ax4.set_ylim(-0.05, 1.1)
    ax4.yaxis.grid(True, alpha=0.2)

    # --- Panel 5: Method Legend / Info (bottom-right) ---
    ax5 = fig.add_subplot(gs[1, 2])
    ax5.set_facecolor(Theme.BG_CARD)
    ax5.axis('off')

    info_text = f"""
    Model Configuration
    -------------------
    Sharer:   {config.get('SHARER_ID', 'N/A').split('/')[-1]}
    Receiver: {config.get('RECEIVER_ID', 'N/A').split('/')[-1]}

    Training
    --------
    Batch Size: {config.get('BATCH_SIZE', 'N/A')}
    Learning Rate: {config.get('lr', 'N/A')}

    Evaluation
    ----------
    MMLU Samples: {config.get('mmlu_sample_size', 'N/A')} per category
    """

    ax5.text(0.1, 0.9, info_text, transform=ax5.transAxes,
             fontsize=9, fontfamily='monospace',
             color=Theme.TEXT_SECONDARY, va='top',
             linespacing=1.5)

    ax5.set_title("Configuration", fontsize=12, fontweight='bold',
                  color=Theme.TEXT_PRIMARY, pad=10, loc='left')

    plt.tight_layout()
    wandb.log({"Evaluation/Training_Summary": wandb.Image(fig)})
    plt.close(fig)

    print("[Viz] Training summary logged.")


# =============================================================================
# NEW VISUALIZATION FUNCTIONS
# =============================================================================

def _compute_category_accuracies(detailed_results):
    """Helper to compute per-category accuracies from detailed results."""
    category_stats = {}
    for result in detailed_results:
        category = _categorize_subject(result['subject'])
        if category not in category_stats:
            category_stats[category] = {'correct': 0, 'total': 0}
        category_stats[category]['total'] += 1
        if result['correct']:
            category_stats[category]['correct'] += 1

    accuracies = {}
    for cat, stats in category_stats.items():
        accuracies[cat] = stats['correct'] / stats['total'] if stats['total'] > 0 else 0

    return accuracies, category_stats


@_safe_viz
def log_category_breakdown(engine, config, detailed_results=None, baseline_results=None):
    """
    Generate per-category MMLU accuracy breakdown comparing methods.
    Shows grouped bars for H2C Bridge vs baselines across categories.

    Args:
        detailed_results: List of dicts from evaluate_accuracy_detailed() for H2C Bridge
        baseline_results: Dict from evaluate_baselines_detailed()
    """
    print("[Viz] Generating comparative category breakdown...")
    _apply_theme()

    # Get detailed results if not provided
    if detailed_results is None:
        _, _, _, detailed_results = engine.mmlu_evaluator.evaluate_accuracy_detailed(engine.mmlu_loader)

    if not detailed_results:
        print("[Viz] Warning: No detailed results available for category breakdown")
        return

    # Build data for all methods - order matters for legend
    methods_data = {}

    # Add baselines first
    if baseline_results:
        if "receiver_only" in baseline_results and baseline_results["receiver_only"].get("details"):
            methods_data["Receiver Only"] = _compute_category_accuracies(
                baseline_results["receiver_only"]["details"]
            )[0]
        if "sharer_only" in baseline_results and baseline_results["sharer_only"].get("details"):
            methods_data["Sharer Only"] = _compute_category_accuracies(
                baseline_results["sharer_only"]["details"]
            )[0]
        if "text_to_text" in baseline_results and baseline_results["text_to_text"].get("details"):
            methods_data["Text-to-Text"] = _compute_category_accuracies(
                baseline_results["text_to_text"]["details"]
            )[0]

    # Add our method last (will be rightmost in grouped bars)
    methods_data["H2C Bridge (Ours)"] = _compute_category_accuracies(detailed_results)[0]

    # Get all categories and sort by H2C Bridge accuracy
    all_categories = list(methods_data["H2C Bridge (Ours)"].keys())
    all_categories.sort(key=lambda c: methods_data["H2C Bridge (Ours)"].get(c, 0), reverse=True)

    # Method colors
    method_colors = {
        "Receiver Only": Theme.TEXT_MUTED,
        "Sharer Only": Theme.ACCENT_PURPLE,
        "Text-to-Text": Theme.ACCENT_WARNING,
        "H2C Bridge (Ours)": Theme.ACCENT_PRIMARY,
    }

    # Plot - wider figure for more methods
    fig, ax = plt.subplots(figsize=(14, 7))
    _format_card(ax, title="Accuracy by Subject Category",
                 subtitle="Comparing H2C Bridge vs All Baselines")

    num_methods = len(methods_data)
    num_categories = len(all_categories)
    bar_height = 0.8 / num_methods

    # Create grouped bars
    for idx, (method_name, cat_accs) in enumerate(methods_data.items()):
        positions = np.arange(num_categories) + idx * bar_height - (num_methods - 1) * bar_height / 2
        accuracies = [cat_accs.get(cat, 0) for cat in all_categories]

        bars = ax.barh(positions, accuracies,
                       height=bar_height * 0.9,
                       color=method_colors.get(method_name, Theme.TEXT_MUTED),
                       alpha=0.85 if "Ours" in method_name else 0.6,
                       label=method_name,
                       edgecolor=Theme.BG_ELEVATED)

    # Y-axis labels
    ax.set_yticks(np.arange(num_categories))
    ax.set_yticklabels(all_categories, fontsize=10)

    # Styling
    ax.set_xlabel("Accuracy", labelpad=10)
    ax.set_xlim(0, 1.0)
    ax.xaxis.grid(True, alpha=0.3)
    ax.yaxis.grid(False)
    ax.spines['left'].set_visible(False)
    ax.tick_params(axis='y', length=0)

    # Legend
    ax.legend(loc='lower right', framealpha=0.9)

    plt.tight_layout()
    wandb.log({"Evaluation/Category_Breakdown_Comparison": wandb.Image(fig)})
    plt.close(fig)

    print("[Viz] Comparative category breakdown logged.")


@_safe_viz
def log_category_breakdown_single(engine, config, detailed_results=None):
    """Generate a single category breakdown (legacy/simple version)."""
    print("[Viz] Generating category breakdown...")
    _apply_theme()

    if detailed_results is None:
        _, _, _, detailed_results = engine.mmlu_evaluator.evaluate_accuracy_detailed(engine.mmlu_loader)

    if not detailed_results:
        return

    accuracies, category_stats = _compute_category_accuracies(detailed_results)
    categories = list(accuracies.keys())
    acc_values = [accuracies[c] for c in categories]
    counts = [category_stats[c]['total'] for c in categories]

    # Sort by accuracy
    sorted_indices = np.argsort(acc_values)[::-1]
    categories = [categories[i] for i in sorted_indices]
    acc_values = [acc_values[i] for i in sorted_indices]
    counts = [counts[i] for i in sorted_indices]

    fig, ax = plt.subplots(figsize=(10, 5))
    _format_card(ax, title="Accuracy by Subject Category",
                 subtitle="Performance breakdown across MMLU domains")

    bars = ax.barh(categories, acc_values, color=Theme.ACCENT_PRIMARY,
                   height=0.6, alpha=0.85, edgecolor=Theme.BG_ELEVATED)

    for bar, acc, count in zip(bars, acc_values, counts):
        ax.text(bar.get_width() + 0.01, bar.get_y() + bar.get_height()/2,
                f"{acc:.1%}",
                ha='left', va='center', fontsize=10, fontweight='bold',
                color=Theme.TEXT_PRIMARY)
        # Count label (inside bar if wide enough)
        if bar.get_width() > 0.1:
            ax.text(0.02, bar.get_y() + bar.get_height()/2,
                    f"n={count}",
                    ha='left', va='center', fontsize=8,
                    color=Theme.BG_DARK, alpha=0.8)

    ax.set_xlabel("Accuracy", labelpad=10)
    ax.set_xlim(0, max(accuracies) * 1.2)
    ax.xaxis.grid(True, alpha=0.2)
    ax.yaxis.grid(False)
    ax.spines['left'].set_visible(False)
    ax.tick_params(axis='y', length=0)

    plt.tight_layout()
    wandb.log({"Evaluation/Category_Breakdown": wandb.Image(fig)})
    plt.close(fig)

    print("[Viz] Category breakdown logged.")


def _build_confusion_matrix(detailed_results):
    """Helper to build a confusion matrix from detailed results."""
    labels = ['A', 'B', 'C', 'D']
    label_to_idx = {l: i for i, l in enumerate(labels)}

    matrix = np.zeros((4, 4), dtype=int)
    invalid_count = 0

    for result in detailed_results:
        true_label = result['label']
        pred_label = result['pred']

        if true_label in label_to_idx:
            true_idx = label_to_idx[true_label]
            if pred_label in label_to_idx:
                pred_idx = label_to_idx[pred_label]
                matrix[true_idx, pred_idx] += 1
            else:
                invalid_count += 1

    # Normalize for percentages
    row_sums = matrix.sum(axis=1, keepdims=True)
    row_sums[row_sums == 0] = 1  # Avoid division by zero
    matrix_pct = matrix / row_sums

    return matrix, matrix_pct, invalid_count


@_safe_viz
def log_confusion_matrix(engine, detailed_results=None, baseline_results=None):
    """
    Generate confusion matrices comparing H2C Bridge vs baselines.
    Shows side-by-side comparison of prediction patterns.

    Args:
        detailed_results: List of dicts from evaluate_accuracy_detailed() for H2C Bridge
        baseline_results: Dict from evaluate_baselines_detailed() with all baselines
    """
    print("[Viz] Generating comparative confusion matrices...")
    _apply_theme()

    # Get detailed results if not provided
    if detailed_results is None:
        _, _, _, detailed_results = engine.mmlu_evaluator.evaluate_accuracy_detailed(engine.mmlu_loader)

    if not detailed_results:
        print("[Viz] Warning: No detailed results available for confusion matrix")
        return

    # Determine how many matrices to show - order matters for display
    methods = {}

    # Add baselines first (in logical order)
    if baseline_results:
        if "receiver_only" in baseline_results and baseline_results["receiver_only"].get("details"):
            methods["Receiver Only"] = baseline_results["receiver_only"]["details"]
        if "sharer_only" in baseline_results and baseline_results["sharer_only"].get("details"):
            methods["Sharer Only"] = baseline_results["sharer_only"]["details"]
        if "text_to_text" in baseline_results and baseline_results["text_to_text"].get("details"):
            methods["Text-to-Text"] = baseline_results["text_to_text"]["details"]

    # Add our method last (rightmost, highlighted)
    methods["H2C Bridge (Ours)"] = detailed_results

    num_methods = len(methods)
    labels = ['A', 'B', 'C', 'D']

    # Create figure with GridSpec for precise control over colorbar placement
    fig_width = 4.5 * num_methods + 1.5  # Space for matrices + colorbar
    fig = plt.figure(figsize=(fig_width, 6))

    # GridSpec: matrices take up most space, colorbar gets a narrow column on right
    gs = fig.add_gridspec(1, num_methods + 1, width_ratios=[1]*num_methods + [0.08], wspace=0.25)

    axes = [fig.add_subplot(gs[0, i]) for i in range(num_methods)]
    cbar_ax = fig.add_subplot(gs[0, num_methods])

    fig.patch.set_facecolor(Theme.BG_DARK)

    # Custom colormap
    cmap = _create_custom_cmap(Theme.GRADIENT_COOL, "confusion")

    for idx, (ax, (method_name, results)) in enumerate(zip(axes, methods.items())):
        matrix, matrix_pct, invalid_count = _build_confusion_matrix(results)

        # Calculate accuracy for title
        accuracy = np.trace(matrix) / matrix.sum() if matrix.sum() > 0 else 0

        im = ax.imshow(matrix_pct, cmap=cmap, vmin=0, vmax=1)

        # Annotations
        for i in range(4):
            for j in range(4):
                val = matrix_pct[i, j]
                count = matrix[i, j]
                text_color = Theme.BG_DARK if val > 0.5 else Theme.TEXT_PRIMARY

                # Highlight diagonal (correct predictions)
                if i == j:
                    ax.add_patch(plt.Rectangle((j-0.5, i-0.5), 1, 1,
                                               fill=False, edgecolor=Theme.ACCENT_SUCCESS,
                                               linewidth=2))

                ax.text(j, i, f"{val:.0%}\n({count})",
                        ha='center', va='center',
                        fontsize=8, fontweight='bold' if i == j else 'normal',
                        color=text_color)

        ax.set_xticks(range(4))
        ax.set_yticks(range(4))
        ax.set_xticklabels(labels, fontsize=10, fontweight='bold')
        ax.set_yticklabels(labels, fontsize=10, fontweight='bold')
        ax.set_xlabel("Predicted", labelpad=6, fontsize=9)
        if idx == 0:
            ax.set_ylabel("Actual", labelpad=6, fontsize=9)

        # Title with accuracy - highlight our method
        is_ours = "Ours" in method_name
        title_color = Theme.ACCENT_PRIMARY if is_ours else Theme.TEXT_PRIMARY
        ax.set_title(f"{method_name}\nAcc: {accuracy:.1%}",
                     fontsize=10, fontweight='bold', pad=8,
                     color=title_color)

        for spine in ax.spines.values():
            spine.set_visible(False)

    # Add colorbar in its dedicated axes
    cbar = fig.colorbar(im, cax=cbar_ax)
    cbar.set_label("Proportion", fontsize=9, color=Theme.TEXT_SECONDARY)

    # Main title
    fig.suptitle("Confusion Matrix Comparison (Row-Normalized)",
                 fontsize=13, fontweight='bold', y=0.98, color=Theme.TEXT_PRIMARY)

    # Adjust margins
    plt.subplots_adjust(top=0.88, bottom=0.12, left=0.05, right=0.95)

    wandb.log({"Evaluation/Confusion_Matrix_Comparison": wandb.Image(fig)})
    plt.close(fig)

    print("[Viz] Comparative confusion matrices logged.")


@_safe_viz
def log_confusion_matrix_single(engine, detailed_results=None):
    """Generate a single confusion matrix (legacy/simple version)."""
    print("[Viz] Generating confusion matrix...")
    _apply_theme()

    if detailed_results is None:
        _, _, _, detailed_results = engine.mmlu_evaluator.evaluate_accuracy_detailed(engine.mmlu_loader)

    if not detailed_results:
        print("[Viz] Warning: No detailed results available")
        return

    matrix, matrix_pct, invalid_count = _build_confusion_matrix(detailed_results)
    labels = ['A', 'B', 'C', 'D']

    fig, ax = plt.subplots(figsize=(8, 7))
    cmap = _create_custom_cmap(Theme.GRADIENT_COOL, "confusion")
    im = ax.imshow(matrix_pct, cmap=cmap, vmin=0, vmax=1)

    for i in range(4):
        for j in range(4):
            val = matrix_pct[i, j]
            count = matrix[i, j]
            text_color = Theme.BG_DARK if val > 0.5 else Theme.TEXT_PRIMARY
            if i == j:
                ax.add_patch(plt.Rectangle((j-0.5, i-0.5), 1, 1,
                                           fill=False, edgecolor=Theme.ACCENT_SUCCESS,
                                           linewidth=2))
            ax.text(j, i, f"{val:.0%}\n({count})",
                    ha='center', va='center',
                    fontsize=10, fontweight='bold' if i == j else 'normal',
                    color=text_color)

    ax.set_xticks(range(4))
    ax.set_yticks(range(4))
    ax.set_xticklabels(labels, fontsize=12, fontweight='bold')
    ax.set_yticklabels(labels, fontsize=12, fontweight='bold')
    ax.set_xlabel("Predicted", labelpad=10, fontweight='bold')
    ax.set_ylabel("Actual", labelpad=10, fontweight='bold')
    ax.set_title("Confusion Matrix (Row-Normalized)",
                 fontsize=14, fontweight='bold', pad=15,
                 color=Theme.TEXT_PRIMARY)

    if invalid_count > 0:
        ax.text(0.5, -0.12, f"Invalid/unparseable predictions: {invalid_count}",
                transform=ax.transAxes, ha='center', fontsize=9,
                color=Theme.ACCENT_WARNING)

    # Colorbar
    cbar = plt.colorbar(im, ax=ax, shrink=0.8)
    cbar.set_label("Proportion", color=Theme.TEXT_SECONDARY)

    for spine in ax.spines.values():
        spine.set_visible(False)

    plt.tight_layout()
    wandb.log({"Evaluation/Confusion_Matrix": wandb.Image(fig)})
    plt.close(fig)

    print("[Viz] Confusion matrix logged.")


@_safe_viz
def log_layer_radar(engine):
    """
    Radar/polar chart showing layer-wise injection importance.
    Provides a visually striking view of which layers contribute most.
    """
    print("[Viz] Generating layer radar chart...")
    _apply_theme()

    bridge = engine.bridge
    k_gates = [layer.gate.clamp(0.0, 1.0).detach().cpu().item()
               for layer in bridge.key_modifiers]
    v_gates = [layer.gate.clamp(0.0, 1.0).detach().cpu().item()
               for layer in bridge.value_modifiers]

    num_layers = len(k_gates)
    if num_layers < 3:
        print("[Viz] Warning: Not enough layers for radar chart (need >= 3)")
        return

    # Angles for radar chart
    angles = np.linspace(0, 2 * np.pi, num_layers, endpoint=False).tolist()
    angles += angles[:1]  # Close the polygon

    k_gates_closed = k_gates + k_gates[:1]
    v_gates_closed = v_gates + v_gates[:1]

    # Plot
    fig, ax = plt.subplots(figsize=(8, 8), subplot_kw=dict(projection='polar'))
    ax.set_facecolor(Theme.BG_CARD)
    fig.patch.set_facecolor(Theme.BG_DARK)

    # Key gates
    ax.plot(angles, k_gates_closed, 'o-', linewidth=2,
            color=Theme.ACCENT_PRIMARY, label='Key Modifier', markersize=6)
    ax.fill(angles, k_gates_closed, alpha=0.2, color=Theme.ACCENT_PRIMARY)

    # Value gates
    ax.plot(angles, v_gates_closed, 's--', linewidth=2,
            color=Theme.ACCENT_WARNING, label='Value Modifier', markersize=5)
    ax.fill(angles, v_gates_closed, alpha=0.15, color=Theme.ACCENT_WARNING)

    # Customize
    ax.set_xticks(angles[:-1])
    ax.set_xticklabels([f"L{i}" for i in range(num_layers)],
                       color=Theme.TEXT_SECONDARY, fontsize=9)
    ax.set_ylim(0, 1.1)
    ax.set_yticks([0.25, 0.5, 0.75, 1.0])
    ax.set_yticklabels(['0.25', '0.5', '0.75', '1.0'],
                       color=Theme.TEXT_MUTED, fontsize=8)
    ax.spines['polar'].set_color(Theme.BORDER)
    ax.grid(color=Theme.GRID, alpha=0.5)

    ax.set_title("Layer-wise Gate Strength",
                 fontsize=12, fontweight='bold', pad=30,
                 color=Theme.TEXT_PRIMARY)
    ax.legend(loc='upper right', bbox_to_anchor=(1.15, 1.1))

    plt.tight_layout()
    wandb.log({"Interpretability/Layer_Radar": wandb.Image(fig)})
    plt.close(fig)

    print("[Viz] Layer radar chart logged.")


@_safe_viz
def log_attention_flow(engine):
    """
    Sankey diagram showing information flow through the bridge.
    Requires plotly to be installed.
    """
    if not PLOTLY_AVAILABLE:
        print("[Viz] Skipping Sankey diagram - plotly not installed")
        return

    print("[Viz] Generating attention flow Sankey diagram...")

    bridge = engine.bridge
    k_gates = [torch.sigmoid(layer.gate).detach().cpu().item()
               for layer in bridge.key_modifiers]
    v_gates = [torch.sigmoid(layer.gate).detach().cpu().item()
               for layer in bridge.value_modifiers]
    num_layers = len(k_gates)

    # Build Sankey data
    # Nodes: Sharer -> Bridge Key/Value modifiers -> Receiver
    labels = (
        ["Sharer Hidden States"] +
        [f"Key Modifier (Layer {i})" for i in range(num_layers)] +
        [f"Value Modifier (Layer {i})" for i in range(num_layers)] +
        ["Receiver KV Cache"]
    )

    # Colors
    node_colors = (
        [Theme.ACCENT_PURPLE] +  # Sharer
        [Theme.ACCENT_PRIMARY] * num_layers +  # K modifiers
        [Theme.ACCENT_WARNING] * num_layers +  # V modifiers
        [Theme.ACCENT_SUCCESS]  # Receiver
    )

    # Links: Sharer -> each modifier, each modifier -> Receiver
    sources = []
    targets = []
    values = []
    link_colors = []

    sharer_idx = 0
    receiver_idx = 1 + 2 * num_layers

    for i in range(num_layers):
        k_mod_idx = 1 + i
        v_mod_idx = 1 + num_layers + i

        # Sharer -> K modifier
        k_strength = abs(k_gates[i]) + 0.1  # Add small offset for visibility
        sources.append(sharer_idx)
        targets.append(k_mod_idx)
        values.append(k_strength)
        link_colors.append(f"rgba(88, 166, 255, {0.3 + k_strength * 0.4})")

        # Sharer -> V modifier
        v_strength = abs(v_gates[i]) + 0.1
        sources.append(sharer_idx)
        targets.append(v_mod_idx)
        values.append(v_strength)
        link_colors.append(f"rgba(210, 153, 34, {0.3 + v_strength * 0.4})")

        # K modifier -> Receiver
        sources.append(k_mod_idx)
        targets.append(receiver_idx)
        values.append(k_strength)
        link_colors.append(f"rgba(88, 166, 255, {0.3 + k_strength * 0.4})")

        # V modifier -> Receiver
        sources.append(v_mod_idx)
        targets.append(receiver_idx)
        values.append(v_strength)
        link_colors.append(f"rgba(210, 153, 34, {0.3 + v_strength * 0.4})")

    # Create Sankey
    fig = go.Figure(data=[go.Sankey(
        node=dict(
            pad=15,
            thickness=20,
            line=dict(color=Theme.BG_DARK, width=0.5),
            label=labels,
            color=node_colors
        ),
        link=dict(
            source=sources,
            target=targets,
            value=values,
            color=link_colors
        )
    )])

    fig.update_layout(
        title_text="H2C Bridge Information Flow",
        title_font=dict(size=16, color=Theme.TEXT_PRIMARY),
        font=dict(size=10, color=Theme.TEXT_SECONDARY),
        paper_bgcolor=Theme.BG_DARK,
        plot_bgcolor=Theme.BG_CARD,
        height=500,
        width=900
    )

    # Log to WandB as HTML
    wandb.log({"Interpretability/Attention_Flow": wandb.Html(fig.to_html())})

    print("[Viz] Attention flow Sankey logged.")


# =============================================================================
# EXECUTION
# =============================================================================

def _generate_all_charts(engine, config, eval_cache, detailed_results, baseline_results, theme_suffix=""):
    """
    Internal function to generate all charts with current theme.
    theme_suffix is added to WandB log keys (e.g., "_light" or "_dark")
    """
    # Temporarily modify wandb.log to add suffix
    original_wandb_log = wandb.log
    def suffixed_log(data, *args, **kwargs):
        if theme_suffix:
            data = {f"{k}{theme_suffix}": v for k, v in data.items()}
        return original_wandb_log(data, *args, **kwargs)
    wandb.log = suffixed_log

    try:
        # --- Performance Charts ---
        log_performance_charts(engine, config, eval_cache=eval_cache, baseline_results=baseline_results)

        # --- Gate Dynamics ---
        log_gate_dynamics(engine)

        # --- Injection Heatmap ---
        log_bridge_heatmap(engine, prompt_text="Explain why the sky is blue.")

        # --- Probability Shift ---
        log_probability_shift(
            engine,
            prompt_text="Question: If x + 2 = 10, what is x?\nA) 5\nB) 8\nC) 12\nD) 2",
            true_label="B"
        )

        # --- Category Breakdown (with baseline comparison) ---
        log_category_breakdown(engine, config,
                               detailed_results=detailed_results,
                               baseline_results=baseline_results)

        # --- Confusion Matrix (with baseline comparison) ---
        log_confusion_matrix(engine,
                             detailed_results=detailed_results,
                             baseline_results=baseline_results)

        # --- Layer Radar Chart ---
        log_layer_radar(engine)

        # --- Attention Flow Sankey ---
        log_attention_flow(engine)

        # --- Qualitative Table (only once, theme doesn't affect it) ---
        # if not theme_suffix:  # Only log table once
        log_qualitative_table(engine)

        # --- Training Summary ---
        log_training_summary(engine, config, eval_cache=eval_cache, baseline_results=baseline_results)
    finally:
        # Restore original wandb.log
        wandb.log = original_wandb_log


def run_all_visualizations(engine, config, themes=("dark", "light"), include_baseline_comparison=True):
    """
    Execute all visualization functions and log to WandB.
    Uses cached evaluation results to avoid redundant computation.

    Args:
        engine: The H2C engine with trained models
        config: Configuration dictionary
        themes: Tuple of themes to generate. Options: "dark", "light", or both.
                Default generates both for publication + presentation use.
        include_baseline_comparison: If True, runs baseline evaluations for
                                     comparative charts (confusion matrix, category breakdown)
    """
    print("\n" + "="*60)
    print("VISUALIZATION SUITE - Publication Ready")
    print("="*60 + "\n")

    # Validate themes
    valid_themes = {"dark", "light"}
    themes = [t for t in themes if t in valid_themes]
    if not themes:
        themes = ["dark"]

    print(f"[Viz] Generating charts for theme(s): {', '.join(themes)}")

    # ==========================================================================
    # STEP 1: Run H2C Bridge evaluation (detailed)
    # ==========================================================================
    print("\n[Viz] Running H2C Bridge evaluation...")
    try:
        h2c_acc, h2c_err, h2c_lat, detailed_results = engine.mmlu_evaluator.evaluate_accuracy_detailed(
            engine.mmlu_loader
        )
        eval_cache = {'acc': h2c_acc, 'err': h2c_err, 'lat': h2c_lat}
        print(f"[Viz] H2C Bridge: Accuracy={h2c_acc:.2%}, Latency={h2c_lat:.3f}s")
    except Exception as e:
        print(f"[Viz] Warning: Detailed evaluation failed ({e}), falling back to basic eval")
        h2c_acc, h2c_err, h2c_lat = engine.mmlu_evaluator.evaluate_accuracy(engine.mmlu_loader)
        eval_cache = {'acc': h2c_acc, 'err': h2c_err, 'lat': h2c_lat}
        detailed_results = None

    # ==========================================================================
    # STEP 2: Run baseline evaluations for comparison (optional)
    # ==========================================================================
    baseline_results = None
    if include_baseline_comparison:
        print("\n[Viz] Running baseline evaluations for comparison...")
        try:
            baseline_results = engine.mmlu_evaluator.evaluate_baselines_detailed(engine.mmlu_loader)
            for mode, results in baseline_results.items():
                print(f"[Viz] {mode}: Accuracy={results['acc']:.2%}")
        except Exception as e:
            print(f"[Viz] Warning: Baseline evaluation failed ({e}), skipping comparisons")
            baseline_results = None

    # ==========================================================================
    # STEP 3: Generate charts for each theme
    # ==========================================================================
    for i, theme_name in enumerate(themes):
        print(f"\n{'='*40}")
        print(f"Generating {theme_name.upper()} theme charts ({i+1}/{len(themes)})")
        print(f"{'='*40}")

        # Switch theme
        Theme.set_theme(theme_name)

        # Determine suffix for WandB keys
        # If only one theme, no suffix. If both, add _dark or _light
        if len(themes) == 1:
            suffix = ""
        else:
            suffix = f"_{theme_name}"

        # Generate all charts
        _generate_all_charts(engine, config, eval_cache, detailed_results, baseline_results, suffix)

    # ==========================================================================
    # DONE
    # ==========================================================================
    print("\n" + "="*60)
    print("All visualizations logged to WandB successfully!")
    print(f"  - Themes generated: {', '.join(themes)}")
    print("  - 10 chart types per theme")
    if include_baseline_comparison:
        print("  - Includes baseline comparisons in confusion matrix & category breakdown")
    if len(themes) > 1:
        print("  - Use '_dark' or '_light' suffix in WandB to filter")
    print("="*60 + "\n")


def run_visualizations_light_only(engine, config):
    """Convenience function to generate only light (publication) theme."""
    run_all_visualizations(engine, config, themes=("light",))


def run_visualizations_dark_only(engine, config):
    """Convenience function to generate only dark (presentation) theme."""
    run_all_visualizations(engine, config, themes=("dark",))

"""# Run visualizations"""

# Reload the MMLU dataset
from torch.utils.data import DataLoader
# Use the new dataset class (Validation split for 'test' equivalent)
config["verbose"] = False
mmlu_dataset = MMLUDataset(split="validation")
# Use the standard collator
mmlu_collator = H2CDataCollator(tok_sharer, tok_receiver)
engine.mmlu_loader = DataLoader(
    mmlu_dataset,
    batch_size=config["BATCH_SIZE"] // 2,
    shuffle=False,
    collate_fn=mmlu_collator
)
# engine.mmlu_evaluator = H2CMMLUEvaluator(engine.sharer, engine.receiver, engine.bridge, tok_receiver, tok_sharer, config, device="cuda")
# Also update the evaluator's reference
# engine.mmlu_evaluator.mmlu_loader = engine.mmlu_loader

# Now run visualizations
run_all_visualizations(engine, config, include_baseline_comparison=True)

# log_qualitative_table(engine)

# log_probability_shift(
#     engine,
#     prompt_text="Question: If x + 2 = 10, what is x?\nA) 5\nB) 8\nC) 12\nD) 2",
#     true_label="B"
# )

# wandb.log({})

# run_mmlu_aux_finetune(engine, steps=500)

# wandb.finish()